{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Settings for this notebook\n",
    "\n",
    "MODEL_URL = 'http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz'\n",
    "model_dir = '/tmp/imagenet'\n",
    "\n",
    "IMAGES_INDEX_URL = 'http://image-net.org/imagenet_data/urls/imagenet_fall11_urls.tgz'\n",
    "images_read_limit = 1000L  # Increase this to read more images\n",
    "\n",
    "# Number of images per batch.\n",
    "# 1 batch corresponds to 1 RDD row.\n",
    "image_batch_size = 3\n",
    "\n",
    "num_top_predictions = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "import re\n",
    "import sys\n",
    "import tarfile\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    print \"TensorFlow is already installed\"\n",
    "except ImportError:\n",
    "    print \"Installing TensorFlow\"\n",
    "    import subprocess\n",
    "    subprocess.check_call([\"/databricks/python/bin/pip\", \"install\", \"https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.0.1-cp27-none-linux_x86_64.whl\"])\n",
    "    from tensorflow.python.platform import gfile\n",
    "    import tensorflow as tf\n",
    "    print \"TensorFlow has been installed on this cluster\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def maybe_download_and_extract():\n",
    "    from six.moves import urllib\n",
    "    dest_directory = model_dir\n",
    "    if not os.path.exists(dest_directory):\n",
    "        os.makedirs(dest_directory)\n",
    "    filename = MODEL_URL.split('/')[-1]\n",
    "    filepath = os.path.join(dest_directory, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "        filepath2, _ = urllib.request.urlretrieve(MODEL_URL, filepath)\n",
    "        print(\"filepath2\", filepath2)\n",
    "        statinfo = os.stat(filepath)\n",
    "        print('Succesfully downloaded', filename, statinfo.st_size, 'bytes.')\n",
    "        tarfile.open(filepath, 'r:gz').extractall(dest_directory)\n",
    "    else:\n",
    "        print('Data already downloaded:', filepath, os.stat(filepath))\n",
    "\n",
    "maybe_download_and_extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_path = os.path.join(model_dir, 'classify_image_graph_def.pb')\n",
    "with tf.python.platform.gfile.FastGFile(model_path, 'rb') as f:\n",
    "    model_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy\n",
    "import random\n",
    "import types\n",
    "from itertools import izip, tee, imap\n",
    "from operator import itemgetter\n",
    "from cStringIO import StringIO\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ConvNet():\n",
    "    IMAGE_PIXELS = 784\n",
    "    NUM_CLASSES = 10\n",
    "    def __init__(self):\n",
    "        self.build(train=False)\n",
    "\n",
    "    def build(self, keep_prob=1.0, lr = 1e-4):\n",
    "        self.x = tf.placeholder(tf.float32, [None, self.IMAGE_PIXELS], name='x-input')\n",
    "        self.y_ = tf.placeholder(tf.float32, [None, self.NUM_CLASSES], name='y-input')\n",
    "\n",
    "        def weight_variable(shape):\n",
    "            return tf.Variable(tf.truncated_normal(shape, stddev=0.1))\n",
    "        def bias_variable(shape):\n",
    "            return tf.Variable(tf.constant(0.1, shape=shape))\n",
    "        def conv2d(x, W):\n",
    "            return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        def max_pool_2x2(x):\n",
    "            return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "        W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "        b_conv1 = bias_variable([32])\n",
    "        x_image = tf.reshape(self.x, [-1,28,28,1])\n",
    "        h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "        h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "        W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "        b_conv2 = bias_variable([64])\n",
    "        h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "        h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "        W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "        b_fc1 = bias_variable([1024])\n",
    "        h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "        W_fc2 = weight_variable([1024, self.NUM_CLASSES])\n",
    "        b_fc2 = bias_variable([self.NUM_CLASSES])\n",
    "\n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "        y = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "        y = tf.nn.softmax(tf.matmul(h_fc1, W_fc2) + b_fc2)\n",
    "\n",
    "        # training\n",
    "        self.loss = -tf.reduce_sum(self.y_*tf.log(y))\n",
    "        self.train = tf.train.AdamOptimizer(lr).minimize(self.loss)\n",
    "\n",
    "        # evaluation\n",
    "        self.evaluate = tf.cast(tf.equal(tf.argmax(y,1), tf.argmax(self.y_,1)), tf.float32)\n",
    "\n",
    "    def transform_batch(self, batch):\n",
    "        x, y_ = iunzip(batch)\n",
    "        return {\n",
    "          self.x: list(x),\n",
    "          self.y_: list(y_)\n",
    "        }\n",
    "\n",
    "class ConvNetEval(ConvNet):\n",
    "    def __init__(self):\n",
    "        self.build(keep_prob = 1.)\n",
    "\n",
    "class ConvNetTrain(ConvNet):\n",
    "    def __init__(self):\n",
    "        self.build(\n",
    "          keep_prob = .5,\n",
    "          lr = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "partitions = 8\n",
    "\n",
    "train_data = sc\\\n",
    "  .parallelize(zip(mnist.train.images, mnist.train.labels), partitions)\\\n",
    "  .cache()     \n",
    "test_data = sc\\\n",
    "  .parallelize(zip(mnist.test.images, mnist.test.labels), partitions)\\\n",
    "  .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master_graph = tf.Graph()\n",
    "with master_graph.as_default(), tf.Session() as session:\n",
    "    dg = DistributedGraph(sc, session, partitions, 50, ConvNetEval)\n",
    "      #dg.params = last_params\n",
    "    for i in xrange(4):\n",
    "        batched_train_data = dg.shuffle_and_batch(train_data, dg.partitions, dg.batch_size, 0.5)\n",
    "        x = dg.train(batched_train_data, worker_epochs=2, graph_cls=ConvNetTrain)\n",
    "        print 'Training loss: %.2f' % x\n",
    "        if i == 0 or ((i+1) % 2 == 0):\n",
    "            print 'Epoch %i Test Acc: %.3f' % (i, dg.evaluate(test_data, graph_cls=ConvNetEval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-csv_2.10:1.3.0 pyspark-shell'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy\n",
    "import random\n",
    "import types\n",
    "from itertools import izip, tee, imap\n",
    "from operator import itemgetter\n",
    "from cStringIO import StringIO\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConvNet():\n",
    "    IMAGE_PIXELS = 784\n",
    "    NUM_CLASSES = 10\n",
    "    def __init__(self):\n",
    "        self.build(train=False)\n",
    "\n",
    "    def build(self, keep_prob=1.0, lr = 1e-4):\n",
    "        self.x = tf.placeholder(tf.float32, [None, self.IMAGE_PIXELS], name='x-input')\n",
    "        self.y_ = tf.placeholder(tf.float32, [None, self.NUM_CLASSES], name='y-input')\n",
    "\n",
    "        def weight_variable(shape):\n",
    "            return tf.Variable(tf.truncated_normal(shape, stddev=0.1))\n",
    "        def bias_variable(shape):\n",
    "            return tf.Variable(tf.constant(0.1, shape=shape))\n",
    "        def conv2d(x, W):\n",
    "            return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        def max_pool_2x2(x):\n",
    "            return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "        W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "        b_conv1 = bias_variable([32])\n",
    "        x_image = tf.reshape(self.x, [-1,28,28,1])\n",
    "        h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "        h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "        W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "        b_conv2 = bias_variable([64])\n",
    "        h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "        h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "        W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "        b_fc1 = bias_variable([1024])\n",
    "        h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "        W_fc2 = weight_variable([1024, self.NUM_CLASSES])\n",
    "        b_fc2 = bias_variable([self.NUM_CLASSES])\n",
    "\n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "        y = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "        y = tf.nn.softmax(tf.matmul(h_fc1, W_fc2) + b_fc2)\n",
    "\n",
    "        # training\n",
    "        self.loss = -tf.reduce_sum(self.y_*tf.log(y))\n",
    "        self.train = tf.train.AdamOptimizer(lr).minimize(self.loss)\n",
    "\n",
    "        # evaluation\n",
    "        self.evaluate = tf.cast(tf.equal(tf.argmax(y,1), tf.argmax(self.y_,1)), tf.float32)\n",
    "\n",
    "    def transform_batch(self, batch):\n",
    "        x, y_ = iunzip(batch)\n",
    "        return {\n",
    "          self.x: list(x),\n",
    "          self.y_: list(y_)\n",
    "        }\n",
    "\n",
    "class ConvNetEval(ConvNet):\n",
    "    def __init__(self):\n",
    "        self.build(keep_prob = 1.)\n",
    "\n",
    "class ConvNetTrain(ConvNet):\n",
    "    def __init__(self):\n",
    "        self.build(\n",
    "          keep_prob = .5,\n",
    "          lr = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/home/canwill/spark2/\")\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"TensorOnSpark\") \\\n",
    "    .config(\"spark.executor.memory\", \"5g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "partitions = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = spark.sparkContext\\\n",
    "  .parallelize(zip(mnist.train.images, mnist.train.labels), partitions)\\\n",
    "  .cache()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = spark.sparkContext\\\n",
    "  .parallelize(zip(mnist.test.images, mnist.test.labels), partitions)\\\n",
    "  .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.38039219,  0.37647063,  0.3019608 ,\n",
       "         0.46274513,  0.2392157 ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.35294119,  0.5411765 ,  0.92156869,\n",
       "         0.92156869,  0.92156869,  0.92156869,  0.92156869,  0.92156869,\n",
       "         0.98431379,  0.98431379,  0.97254908,  0.99607849,  0.96078438,\n",
       "         0.92156869,  0.74509805,  0.08235294,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.54901963,\n",
       "         0.98431379,  0.99607849,  0.99607849,  0.99607849,  0.99607849,\n",
       "         0.99607849,  0.99607849,  0.99607849,  0.99607849,  0.99607849,\n",
       "         0.99607849,  0.99607849,  0.99607849,  0.99607849,  0.99607849,\n",
       "         0.74117649,  0.09019608,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.88627458,  0.99607849,  0.81568635,\n",
       "         0.78039223,  0.78039223,  0.78039223,  0.78039223,  0.54509807,\n",
       "         0.2392157 ,  0.2392157 ,  0.2392157 ,  0.2392157 ,  0.2392157 ,\n",
       "         0.50196081,  0.8705883 ,  0.99607849,  0.99607849,  0.74117649,\n",
       "         0.08235294,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.14901961,  0.32156864,  0.0509804 ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.13333334,\n",
       "         0.83529419,  0.99607849,  0.99607849,  0.45098042,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.32941177,  0.99607849,\n",
       "         0.99607849,  0.91764712,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.32941177,  0.99607849,  0.99607849,  0.91764712,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.41568631,  0.6156863 ,\n",
       "         0.99607849,  0.99607849,  0.95294124,  0.20000002,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.09803922,  0.45882356,  0.89411771,  0.89411771,\n",
       "         0.89411771,  0.99215692,  0.99607849,  0.99607849,  0.99607849,\n",
       "         0.99607849,  0.94117653,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.26666668,  0.4666667 ,  0.86274517,\n",
       "         0.99607849,  0.99607849,  0.99607849,  0.99607849,  0.99607849,\n",
       "         0.99607849,  0.99607849,  0.99607849,  0.99607849,  0.55686277,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.14509805,  0.73333335,\n",
       "         0.99215692,  0.99607849,  0.99607849,  0.99607849,  0.87450987,\n",
       "         0.80784321,  0.80784321,  0.29411766,  0.26666668,  0.84313732,\n",
       "         0.99607849,  0.99607849,  0.45882356,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.44313729,  0.8588236 ,  0.99607849,  0.94901967,  0.89019614,\n",
       "         0.45098042,  0.34901962,  0.12156864,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.7843138 ,  0.99607849,  0.9450981 ,\n",
       "         0.16078432,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.66274512,  0.99607849,\n",
       "         0.6901961 ,  0.24313727,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.18823531,\n",
       "         0.90588242,  0.99607849,  0.91764712,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.07058824,  0.48627454,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.32941177,  0.99607849,  0.99607849,\n",
       "         0.65098041,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.54509807,  0.99607849,  0.9333334 ,  0.22352943,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.82352948,  0.98039222,  0.99607849,\n",
       "         0.65882355,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.94901967,  0.99607849,  0.93725497,  0.22352943,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.34901962,  0.98431379,  0.9450981 ,\n",
       "         0.33725491,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.01960784,\n",
       "         0.80784321,  0.96470594,  0.6156863 ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.01568628,  0.45882356,  0.27058825,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ], dtype=float32),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master_graph = tf.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DistributedGraph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-f3b9eba343f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mmaster_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDistributedGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConvNetEval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mbatched_train_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle_and_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_train_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mConvNetTrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DistributedGraph' is not defined"
     ]
    }
   ],
   "source": [
    "with master_graph.as_default(), tf.Session() as session:\n",
    "    dg = DistributedGraph(spark.sparkContext, session, partitions, 50, ConvNetEval)\n",
    "    for i in xrange(4):\n",
    "        batched_train_data = dg.shuffle_and_batch(train_data, dg.partitions, dg.batch_size, 0.5)\n",
    "        x = dg.train(batched_train_data, worker_epochs=2, graph_cls=ConvNetTrain)\n",
    "        print 'Training loss: %.2f' % x\n",
    "        if i == 0 or ((i+1) % 2 == 0):\n",
    "            print 'Epoch %i Test Acc: %.3f' % (i, dg.evaluate(test_data, graph_cls=ConvNetEval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/home/canwill/spark2/\")\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"TensorOnSpark\") \\\n",
    "    .config(\"spark.executor.memory\", \"5g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit # Create columns of *literal* value\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = spark.read.csv('data/train.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PassengerId',\n",
       " 'Survived',\n",
       " 'Pclass',\n",
       " 'Name',\n",
       " 'Sex',\n",
       " 'Age',\n",
       " 'SibSp',\n",
       " 'Parch',\n",
       " 'Ticket',\n",
       " 'Fare',\n",
       " 'Cabin',\n",
       " 'Embarked']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = spark.read.csv('data/test.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = train.withColumn('Mark', lit('train'))\n",
    "test = (test.withColumn('Survived',lit(0))\n",
    "                .withColumn('Mark', lit('test')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Append Test data to Train data\n",
    "df = train.unionAll(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"AgeTmp\", df[\"Age\"].cast(\"float\")) \\\n",
    "    .drop(\"Age\") \\\n",
    "    .withColumnRenamed(\"AgeTmp\", \"Age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_anytype(df, colnames, typename):\n",
    "    for colname in colnames:\n",
    "        df = df.withColumn(\"tmp\", df[colname].cast(typename)) \\\n",
    "        .drop(colname) \\\n",
    "        .withColumnRenamed(\"tmp\", colname)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "intCols = ['PassengerId', 'Pclass', 'SibSp', 'Parch', 'Survived']\n",
    "floatCols = ['Age', 'Fare']\n",
    "\n",
    "df = to_anytype(df, intCols, \"integer\")\n",
    "df = to_anytype(df, floatCols, \"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| age|count|\n",
      "+----+-----+\n",
      "|null|  177|\n",
      "|   0|  283|\n",
      "|0.42|    1|\n",
      "|0.67|    1|\n",
      "|0.75|    2|\n",
      "|0.83|    2|\n",
      "|0.92|    1|\n",
      "|   1|  117|\n",
      "|  10|    2|\n",
      "|  11|    4|\n",
      "|  12|    1|\n",
      "|  13|    2|\n",
      "|  14|    6|\n",
      "|14.5|    1|\n",
      "|  15|    5|\n",
      "|  16|   17|\n",
      "|  17|   13|\n",
      "|  18|   26|\n",
      "|  19|   25|\n",
      "|   2|   24|\n",
      "|  20|   15|\n",
      "|20.5|    1|\n",
      "|  21|   24|\n",
      "|  22|   27|\n",
      "|  23|   15|\n",
      "|23.5|    1|\n",
      "|  24|   30|\n",
      "|24.5|    1|\n",
      "|  25|   23|\n",
      "|  26|   18|\n",
      "|  27|   18|\n",
      "|  28|   25|\n",
      "|28.5|    2|\n",
      "|  29|   20|\n",
      "|   3|   10|\n",
      "|  30|   25|\n",
      "|30.5|    2|\n",
      "|  31|   17|\n",
      "|  32|   18|\n",
      "|32.5|    2|\n",
      "|  33|   15|\n",
      "|  34|   15|\n",
      "|34.5|    1|\n",
      "|  35|   18|\n",
      "|  36|   22|\n",
      "|36.5|    1|\n",
      "|  37|    6|\n",
      "|  38|   11|\n",
      "|  39|   14|\n",
      "|   4|   14|\n",
      "|  40|   13|\n",
      "|40.5|    2|\n",
      "|  41|    6|\n",
      "|  42|   13|\n",
      "|  43|    5|\n",
      "|  44|    9|\n",
      "|  45|   12|\n",
      "|45.5|    2|\n",
      "|  46|    3|\n",
      "|  47|    9|\n",
      "|  48|    9|\n",
      "|  49|    6|\n",
      "|   5|    5|\n",
      "|  50|   10|\n",
      "|  51|    7|\n",
      "|  52|    6|\n",
      "|  53|    1|\n",
      "|  54|    8|\n",
      "|  55|    2|\n",
      "|55.5|    1|\n",
      "|  56|    4|\n",
      "|  57|    2|\n",
      "|  58|    5|\n",
      "|  59|    2|\n",
      "|   6|    3|\n",
      "|  60|    4|\n",
      "|  61|    3|\n",
      "|  62|    4|\n",
      "|  63|    2|\n",
      "|  64|    2|\n",
      "|  65|    3|\n",
      "|  66|    1|\n",
      "|   7|    3|\n",
      "|  70|    2|\n",
      "|70.5|    1|\n",
      "|  71|    2|\n",
      "|  74|    1|\n",
      "|   8|    6|\n",
      "|  80|    1|\n",
      "|   9|    8|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "age_hist = spark.sql(\n",
    "    \"SELECT Age AS age, \\\n",
    "            count(*) AS count \\\n",
    "    FROM train \\\n",
    "    GROUP BY Age \\\n",
    "    ORDER BY Age\")\n",
    "age_hist.show(n=age_hist.count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+-----+\n",
      "|bucket_floor|bucket_name|count|\n",
      "+------------+-----------+-----+\n",
      "|        null|       null|  177|\n",
      "|           0|     0 to 5|  455|\n",
      "|           5|    5 to 10|   25|\n",
      "|          10|   10 to 15|   16|\n",
      "|          15|   15 to 20|   86|\n",
      "|          20|   20 to 25|  114|\n",
      "|          25|   25 to 30|  106|\n",
      "|          30|   30 to 35|   95|\n",
      "|          35|   35 to 40|   72|\n",
      "|          40|   40 to 45|   48|\n",
      "|          45|   45 to 50|   41|\n",
      "|          50|   50 to 55|   32|\n",
      "|          55|   55 to 60|   16|\n",
      "|          60|   60 to 65|   15|\n",
      "|          65|   65 to 70|    4|\n",
      "|          70|   70 to 75|    6|\n",
      "|          80|   80 to 85|    1|\n",
      "+------------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "age_hist = spark.sql(\n",
    "    \"SELECT bucket_floor, \\\n",
    "        CONCAT(bucket_floor, ' to ', bucket_ceiling) as bucket_name, \\\n",
    "        count(*) as count \\\n",
    "     FROM ( \\\n",
    "        SELECT floor(Age/5.00)*5 as bucket_floor, \\\n",
    "            floor(Age/5.00)*5 + 5 as bucket_ceiling \\\n",
    "        FROM train \\\n",
    "     ) a \\\n",
    "     GROUP BY 1, 2 \\\n",
    "     ORDER BY 1\")\n",
    "\n",
    "age_hist.show(n=age_hist.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numVars = ['Survived','Age','SibSp','Parch','Fare']\n",
    "stringVars = ['Cabin', 'Embarked', 'Pclass', 'Sex']\n",
    "\n",
    "def countNull(df,var):\n",
    "    return df.where(df[var].isNull()).count()\n",
    "\n",
    "def countEmptyString(df,var):\n",
    "    return df[df[var].isin(\"\")].count()\n",
    "\n",
    "def countZero(df,var):\n",
    "    return df[df[var].isin(0)].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Age': 177,\n",
       " 'Cabin': 687,\n",
       " 'Embarked': 2,\n",
       " 'Fare': 418,\n",
       " 'Mark': 0,\n",
       " 'Name': 0,\n",
       " 'Parch': 122,\n",
       " 'PassengerId': 0,\n",
       " 'Pclass': 418,\n",
       " 'Sex': 86,\n",
       " 'SibSp': 0,\n",
       " 'Survived': 0,\n",
       " 'Ticket': 1}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "missing = {var: countNull(df,var) for var in df.columns}\n",
    "missing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Age': 0,\n",
       " 'Cabin': 0,\n",
       " 'Embarked': 0,\n",
       " 'Fare': 0,\n",
       " 'Mark': 0,\n",
       " 'Name': 0,\n",
       " 'Parch': 0,\n",
       " 'PassengerId': 0,\n",
       " 'Pclass': 0,\n",
       " 'Sex': 0,\n",
       " 'SibSp': 0,\n",
       " 'Survived': 0,\n",
       " 'Ticket': 0}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "missing = {var: countEmptyString(df, var) for var in df.columns}\n",
    "missing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Age': 283,\n",
       " 'Cabin': 0,\n",
       " 'Embarked': 418,\n",
       " 'Fare': 15,\n",
       " 'Mark': 0,\n",
       " 'Name': 0,\n",
       " 'Parch': 678,\n",
       " 'PassengerId': 0,\n",
       " 'Pclass': 0,\n",
       " 'Sex': 0,\n",
       " 'SibSp': 932,\n",
       " 'Survived': 549,\n",
       " 'Ticket': 2}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing = {var: countZero(df, var) for var in df.columns}\n",
    "missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(avg(Age)=18.897676678433644)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age_mean = df.groupBy().mean('Age').first()\n",
    "age_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18.897676678433644, 32.20420804114722)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "age_mean = df.groupBy().mean('Age').first()[0]\n",
    "fare_mean = df.groupBy().mean('Fare').first()[0]\n",
    "age_mean, fare_mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.na.fill({'Age':age_mean,'Fare':fare_mean})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+\n",
      "|                Name|          Title|\n",
      "+--------------------+---------------+\n",
      "|Braund, Mr. Owen ...|     Braund, Mr|\n",
      "|Cumings, Mrs. Joh...|   Cumings, Mrs|\n",
      "|Heikkinen, Miss. ...|Heikkinen, Miss|\n",
      "|Futrelle, Mrs. Ja...|  Futrelle, Mrs|\n",
      "|Allen, Mr. Willia...|      Allen, Mr|\n",
      "+--------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    " \n",
    "## create user defined function to extract title\n",
    "getTitle = udf(lambda name: name.split('.')[0].strip(), StringType())\n",
    "df = df.withColumn('Title', getTitle(df['Name']))\n",
    " \n",
    "df.select('Name','Title').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                Name|Title|\n",
      "+--------------------+-----+\n",
      "|Braund, Mr. Owen ...|   Mr|\n",
      "|Cumings, Mrs. Joh...|  Mrs|\n",
      "|Heikkinen, Miss. ...| Miss|\n",
      "|Futrelle, Mrs. Ja...|  Mrs|\n",
      "|Allen, Mr. Willia...|   Mr|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "getTitle = udf(lambda name: name.split('.')[0].split(',')[1].strip(), StringType())\n",
    "df = df.withColumn('Title', getTitle(df['Name']))\n",
    " \n",
    "df.select('Name','Title').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrameNaFunctions\n",
    "from pyspark.sql.functions import lit # Create columns of *literal* value\n",
    "from pyspark.sql.functions import col # Returns a Column based on the \n",
    "                                      # given column name\n",
    "from pyspark.ml.feature import StringIndexer #label encoding\n",
    "from pyspark.ml import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o460.transform.\n: java.lang.NullPointerException\n\tat org.apache.spark.sql.types.Metadata$.org$apache$spark$sql$types$Metadata$$hash(Metadata.scala:219)\n\tat org.apache.spark.sql.types.Metadata$$anonfun$org$apache$spark$sql$types$Metadata$$hash$2.apply(Metadata.scala:207)\n\tat org.apache.spark.sql.types.Metadata$$anonfun$org$apache$spark$sql$types$Metadata$$hash$2.apply(Metadata.scala:207)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.types.Metadata$.org$apache$spark$sql$types$Metadata$$hash(Metadata.scala:207)\n\tat org.apache.spark.sql.types.Metadata$$anonfun$org$apache$spark$sql$types$Metadata$$hash$1.apply(Metadata.scala:204)\n\tat org.apache.spark.sql.types.Metadata$$anonfun$org$apache$spark$sql$types$Metadata$$hash$1.apply(Metadata.scala:204)\n\tat scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)\n\tat scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)\n\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n\tat scala.collection.immutable.Map$Map3.foreach(Map.scala:161)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n\tat scala.collection.MapLike$MappedValues.foreach(MapLike.scala:245)\n\tat scala.util.hashing.MurmurHash3.unorderedHash(MurmurHash3.scala:91)\n\tat scala.util.hashing.MurmurHash3$.mapHash(MurmurHash3.scala:222)\n\tat scala.collection.GenMapLike$class.hashCode(GenMapLike.scala:35)\n\tat scala.collection.AbstractMap.hashCode(Map.scala:59)\n\tat scala.runtime.ScalaRunTime$.hash(ScalaRunTime.scala:206)\n\tat org.apache.spark.sql.types.Metadata$.org$apache$spark$sql$types$Metadata$$hash(Metadata.scala:204)\n\tat org.apache.spark.sql.types.Metadata$$anonfun$org$apache$spark$sql$types$Metadata$$hash$1.apply(Metadata.scala:204)\n\tat org.apache.spark.sql.types.Metadata$$anonfun$org$apache$spark$sql$types$Metadata$$hash$1.apply(Metadata.scala:204)\n\tat scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)\n\tat scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)\n\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n\tat scala.collection.immutable.Map$Map1.foreach(Map.scala:116)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n\tat scala.collection.MapLike$MappedValues.foreach(MapLike.scala:245)\n\tat scala.util.hashing.MurmurHash3.unorderedHash(MurmurHash3.scala:91)\n\tat scala.util.hashing.MurmurHash3$.mapHash(MurmurHash3.scala:222)\n\tat scala.collection.GenMapLike$class.hashCode(GenMapLike.scala:35)\n\tat scala.collection.AbstractMap.hashCode(Map.scala:59)\n\tat scala.runtime.ScalaRunTime$.hash(ScalaRunTime.scala:206)\n\tat org.apache.spark.sql.types.Metadata$.org$apache$spark$sql$types$Metadata$$hash(Metadata.scala:204)\n\tat org.apache.spark.sql.types.Metadata._hashCode$lzycompute(Metadata.scala:107)\n\tat org.apache.spark.sql.types.Metadata._hashCode(Metadata.scala:107)\n\tat org.apache.spark.sql.types.Metadata.hashCode(Metadata.scala:108)\n\tat org.apache.spark.sql.catalyst.expressions.AttributeReference.hashCode(namedExpressions.scala:249)\n\tat scala.runtime.ScalaRunTime$.hash(ScalaRunTime.scala:206)\n\tat scala.collection.immutable.HashSet.elemHashCode(HashSet.scala:177)\n\tat scala.collection.immutable.HashSet.computeHash(HashSet.scala:186)\n\tat scala.collection.immutable.HashSet.$plus(HashSet.scala:84)\n\tat scala.collection.immutable.HashSet.$plus(HashSet.scala:35)\n\tat scala.collection.mutable.SetBuilder.$plus$eq(SetBuilder.scala:22)\n\tat scala.collection.mutable.SetBuilder.$plus$eq(SetBuilder.scala:20)\n\tat scala.collection.generic.Growable$class.loop$1(Growable.scala:53)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:57)\n\tat scala.collection.mutable.SetBuilder.$plus$plus$eq(SetBuilder.scala:20)\n\tat scala.collection.TraversableLike$class.to(TraversableLike.scala:590)\n\tat scala.collection.AbstractTraversable.to(Traversable.scala:104)\n\tat scala.collection.TraversableOnce$class.toSet(TraversableOnce.scala:304)\n\tat scala.collection.AbstractTraversable.toSet(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.containsChild$lzycompute(TreeNode.scala:89)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.containsChild(TreeNode.scala:89)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5$$anonfun$apply$11.apply(TreeNode.scala:359)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:285)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:358)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionDown$1(QueryPlan.scala:248)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:258)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$6.apply(QueryPlan.scala:267)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDown(QueryPlan.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressions(QueryPlan.scala:236)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveDeserializer$$anonfun$apply$32.applyOrElse(Analyzer.scala:2027)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveDeserializer$$anonfun$apply$32.applyOrElse(Analyzer.scala:2023)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:61)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:61)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:60)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveDeserializer$.apply(Analyzer.scala:2023)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveDeserializer$.apply(Analyzer.scala:2022)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:85)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:82)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:82)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:74)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:74)\n\tat org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.resolveAndBind(ExpressionEncoder.scala:258)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:209)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:2822)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1121)\n\tat org.apache.spark.ml.feature.StringIndexerModel.transform(StringIndexer.scala:185)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-dcf4925e1058>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m## index Sex variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStringIndexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Sex'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Sex_indexed'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf_indexed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Sex'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Sex_indexed'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Sex'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/canwill/spark2/python/pyspark/ml/base.pyc\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/canwill/spark2/python/pyspark/ml/wrapper.pyc\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/canwill/spark2/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/canwill/spark2/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/canwill/spark2/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o460.transform.\n: java.lang.NullPointerException\n\tat org.apache.spark.sql.types.Metadata$.org$apache$spark$sql$types$Metadata$$hash(Metadata.scala:219)\n\tat org.apache.spark.sql.types.Metadata$$anonfun$org$apache$spark$sql$types$Metadata$$hash$2.apply(Metadata.scala:207)\n\tat org.apache.spark.sql.types.Metadata$$anonfun$org$apache$spark$sql$types$Metadata$$hash$2.apply(Metadata.scala:207)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.types.Metadata$.org$apache$spark$sql$types$Metadata$$hash(Metadata.scala:207)\n\tat org.apache.spark.sql.types.Metadata$$anonfun$org$apache$spark$sql$types$Metadata$$hash$1.apply(Metadata.scala:204)\n\tat org.apache.spark.sql.types.Metadata$$anonfun$org$apache$spark$sql$types$Metadata$$hash$1.apply(Metadata.scala:204)\n\tat scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)\n\tat scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)\n\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n\tat scala.collection.immutable.Map$Map3.foreach(Map.scala:161)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n\tat scala.collection.MapLike$MappedValues.foreach(MapLike.scala:245)\n\tat scala.util.hashing.MurmurHash3.unorderedHash(MurmurHash3.scala:91)\n\tat scala.util.hashing.MurmurHash3$.mapHash(MurmurHash3.scala:222)\n\tat scala.collection.GenMapLike$class.hashCode(GenMapLike.scala:35)\n\tat scala.collection.AbstractMap.hashCode(Map.scala:59)\n\tat scala.runtime.ScalaRunTime$.hash(ScalaRunTime.scala:206)\n\tat org.apache.spark.sql.types.Metadata$.org$apache$spark$sql$types$Metadata$$hash(Metadata.scala:204)\n\tat org.apache.spark.sql.types.Metadata$$anonfun$org$apache$spark$sql$types$Metadata$$hash$1.apply(Metadata.scala:204)\n\tat org.apache.spark.sql.types.Metadata$$anonfun$org$apache$spark$sql$types$Metadata$$hash$1.apply(Metadata.scala:204)\n\tat scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)\n\tat scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245)\n\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n\tat scala.collection.immutable.Map$Map1.foreach(Map.scala:116)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n\tat scala.collection.MapLike$MappedValues.foreach(MapLike.scala:245)\n\tat scala.util.hashing.MurmurHash3.unorderedHash(MurmurHash3.scala:91)\n\tat scala.util.hashing.MurmurHash3$.mapHash(MurmurHash3.scala:222)\n\tat scala.collection.GenMapLike$class.hashCode(GenMapLike.scala:35)\n\tat scala.collection.AbstractMap.hashCode(Map.scala:59)\n\tat scala.runtime.ScalaRunTime$.hash(ScalaRunTime.scala:206)\n\tat org.apache.spark.sql.types.Metadata$.org$apache$spark$sql$types$Metadata$$hash(Metadata.scala:204)\n\tat org.apache.spark.sql.types.Metadata._hashCode$lzycompute(Metadata.scala:107)\n\tat org.apache.spark.sql.types.Metadata._hashCode(Metadata.scala:107)\n\tat org.apache.spark.sql.types.Metadata.hashCode(Metadata.scala:108)\n\tat org.apache.spark.sql.catalyst.expressions.AttributeReference.hashCode(namedExpressions.scala:249)\n\tat scala.runtime.ScalaRunTime$.hash(ScalaRunTime.scala:206)\n\tat scala.collection.immutable.HashSet.elemHashCode(HashSet.scala:177)\n\tat scala.collection.immutable.HashSet.computeHash(HashSet.scala:186)\n\tat scala.collection.immutable.HashSet.$plus(HashSet.scala:84)\n\tat scala.collection.immutable.HashSet.$plus(HashSet.scala:35)\n\tat scala.collection.mutable.SetBuilder.$plus$eq(SetBuilder.scala:22)\n\tat scala.collection.mutable.SetBuilder.$plus$eq(SetBuilder.scala:20)\n\tat scala.collection.generic.Growable$class.loop$1(Growable.scala:53)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:57)\n\tat scala.collection.mutable.SetBuilder.$plus$plus$eq(SetBuilder.scala:20)\n\tat scala.collection.TraversableLike$class.to(TraversableLike.scala:590)\n\tat scala.collection.AbstractTraversable.to(Traversable.scala:104)\n\tat scala.collection.TraversableOnce$class.toSet(TraversableOnce.scala:304)\n\tat scala.collection.AbstractTraversable.toSet(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.containsChild$lzycompute(TreeNode.scala:89)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.containsChild(TreeNode.scala:89)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5$$anonfun$apply$11.apply(TreeNode.scala:359)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:285)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:358)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionDown$1(QueryPlan.scala:248)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:258)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$6.apply(QueryPlan.scala:267)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDown(QueryPlan.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressions(QueryPlan.scala:236)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveDeserializer$$anonfun$apply$32.applyOrElse(Analyzer.scala:2027)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveDeserializer$$anonfun$apply$32.applyOrElse(Analyzer.scala:2023)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:61)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:61)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:60)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveDeserializer$.apply(Analyzer.scala:2023)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveDeserializer$.apply(Analyzer.scala:2022)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:85)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:82)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:82)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:74)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:74)\n\tat org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.resolveAndBind(ExpressionEncoder.scala:258)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:209)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:2822)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1121)\n\tat org.apache.spark.ml.feature.StringIndexerModel.transform(StringIndexer.scala:185)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "catVars = ['Pclass','Sex','Embarked','Title']\n",
    " \n",
    "## index Sex variable\n",
    "si = StringIndexer(inputCol = 'Sex', outputCol = 'Sex_indexed')\n",
    "df_indexed = si.fit(df).transform(df).drop('Sex').withColumnRenamed('Sex_indexed','Sex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib import learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = learn.LinearClassifier(n_classes=2, \n",
    "feature_columns=learn.infer_real_valued_columns_from_input(X_train), optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.05))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
