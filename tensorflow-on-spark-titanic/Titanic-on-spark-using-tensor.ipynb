{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages databricks:tensorframes:0.2.7-s_2.11 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections # For frequency counting\n",
    "import findspark\n",
    "findspark.init(\"/home/canwill/spark2/\")\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import DataFrameNaFunctions\n",
    "from pyspark.sql.functions import lit # Create columns of *literal* value\n",
    "from pyspark.sql.functions import col # Returns a Column based on the \n",
    "                                      # given column name\n",
    "from pyspark.ml.feature import StringIndexer #label encoding\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "#sc = pyspark.SparkContext(appName=\"helloworld\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"TensorOnSpark\") \\\n",
    "    .config(\"spark.executor.memory\", \"10g\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "spark.conf.set(\"spark.sql.caseSensitive\", \"false\");\n",
    "spark.sql(\"set spark.sql.caseSensitive=false\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainDF = spark.read.csv(\"data/train.csv\", header=\"true\")\n",
    "testDF = spark.read.csv(\"data/test.csv\", header=\"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combine train and test data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Add Survived column to test, and dataset name as a column\n",
    "trainDF = trainDF.withColumn('Mark', lit('train'))\n",
    "testDF = (testDF.withColumn('Survived',lit(0))\n",
    "                .withColumn('Mark', lit('test')))\n",
    "testDF = testDF[trainDF.columns]\n",
    "\n",
    "## Append Test data to Train data\n",
    "df = trainDF.unionAll(testDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps in a Machine Learning Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Data Collection\n",
    "* Data Preprocessing\n",
    "* Feature Engineering\n",
    "* Data format translation\n",
    "* Modeling\n",
    "* Evaluation and Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Combiniing Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Statistical Summary\n",
    "* Histograms\n",
    "* Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the schema?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which ones are numeric?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the variables which should be numeric (float or integer):\n",
    "\n",
    "* PassengerId: Integer\n",
    "* Pclass: Integer\n",
    "* SibSp: Integer\n",
    "* Parch: Integer\n",
    "* Survived: Integer\n",
    "* Age: Float\n",
    "* Fare: Float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here is an example\n",
    "df = df.withColumn(\"AgeTmp\", df[\"Age\"].cast(\"float\")) \\\n",
    "    .drop(\"Age\") \\\n",
    "    .withColumnRenamed(\"AgeTmp\", \"Age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's define function\n",
    "def to_anytype(df, colnames, typename):\n",
    "    for colname in colnames:\n",
    "        df = df.withColumn(\"tmp\", df[colname].cast(typename)) \\\n",
    "        .drop(colname) \\\n",
    "        .withColumnRenamed(\"tmp\", colname)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "intCols = ['PassengerId', 'Pclass', 'SibSp', 'Parch', 'Survived']\n",
    "floatCols = ['Age', 'Fare']\n",
    "\n",
    "df = to_anytype(df, intCols, \"integer\")\n",
    "df = to_anytype(df, floatCols, \"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.describe('Age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.describe(['Age', 'Name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.describe(trainDF.columns).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.describe(trainDF.columns[1:4]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.describe(trainDF.columns[5:8]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.describe(trainDF.columns[9:12]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We need the frequency count of various levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "age_hist = spark.sql(\n",
    "    \"SELECT Age AS age, \\\n",
    "            count(*) AS count \\\n",
    "    FROM train \\\n",
    "    GROUP BY Age \\\n",
    "    ORDER BY Age\")\n",
    "age_hist.show(n=age_hist.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "age_hist = spark.sql(\n",
    "    \"SELECT bucket_floor, \\\n",
    "        CONCAT(bucket_floor, ' to ', bucket_ceiling) as bucket_name, \\\n",
    "        count(*) as count \\\n",
    "     FROM ( \\\n",
    "        SELECT floor(Age/5.00)*5 as bucket_floor, \\\n",
    "            floor(Age/5.00)*5 + 5 as bucket_ceiling \\\n",
    "        FROM train \\\n",
    "     ) a \\\n",
    "     GROUP BY 1, 2 \\\n",
    "     ORDER BY 1\")\n",
    "\n",
    "age_hist.show(n=age_hist.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_column(df, colname):\n",
    "    coldata = df.rdd.map(lambda r: r[colname]).collect()\n",
    "    coldata = ['None' if v is None else v for v in coldata] #replace None values\n",
    "    return(coldata)\n",
    "\n",
    "age = get_column(age_hist, \"bucket_name\")\n",
    "count = get_column(age_hist, \"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "barplt = sns.barplot(age, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "barplt = sns.barplot(age, count)\n",
    "for item in barplt.get_xticklabels():\n",
    "    item.set_rotation(45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogram Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_column(df, colname):\n",
    "    coldata = df.rdd.map(lambda r: r[colname]).collect()\n",
    "    coldata = ['None' if v is None else v for v in coldata] #replace None values\n",
    "    return(coldata)\n",
    "\n",
    "def histplot(dfname, colname, binsize):\n",
    "    binsize = str(binsize)\n",
    "    dfname.createOrReplaceTempView(\"tmpDF\")\n",
    "    hist_query = \"SELECT bucket_floor, \\\n",
    "        CONCAT(bucket_floor, ' to ', bucket_ceiling) as bucket_name, \\\n",
    "        count(*) as count \\\n",
    "     FROM ( \\\n",
    "        SELECT floor(\" + colname + \"/\" + binsize + \")*\" + binsize + \" as bucket_floor, \\\n",
    "            floor(\" + colname + \"/\" + binsize + \")*\" + binsize + \" + \" + binsize + \" as bucket_ceiling \\\n",
    "        FROM tmpDF \\\n",
    "     ) a \\\n",
    "     GROUP BY 1, 2 \\\n",
    "     ORDER BY 1\"\n",
    "    hist_data = spark.sql(hist_query)\n",
    "    xvar = get_column(hist_data, \"bucket_name\")\n",
    "    count = get_column(hist_data, \"count\")\n",
    "    barplt = sns.barplot(xvar, count)\n",
    "    for item in barplt.get_xticklabels():\n",
    "        item.set_rotation(45)\n",
    "    return(barplt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "histplot(df, \"Age\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "histplot(df, \"Age\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Play with various binsizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "histplot(df, \"Age\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "histplot(df, \"Survived\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "histplot(df, \"Survived\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "histplot(df, \"Pclass\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "histplot(df, \"SibSp\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "histplot(df, \"Parch\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "histplot(df, \"Age\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "histplot(df, \"Fare\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "histplot(df, \"Fare\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test with a categorical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "histplot(trainDF, \"Embarked\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def histplot_s(df, colname):\n",
    "    xvar = get_column(df, colname)\n",
    "    counter = collections.Counter(xvar)\n",
    "    barplt = sns.barplot(list(counter.keys()), list(counter.values()))\n",
    "    for item in barplt.get_xticklabels():\n",
    "        item.set_rotation(45)\n",
    "    return(barplt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "histplot_s(df, \"Sex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "histplot_s(df, \"Embarked\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.corr(\"Age\", \"Fare\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.corr(\"Age\", \"Survived\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.corr(\"Fare\", \"Survived\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, only *pearson* is supported."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Value Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numVars = ['Survived','Age','SibSp','Parch','Fare']\n",
    "stringVars = ['Cabin', 'Embarked', 'Pclass', 'Sex']\n",
    "\n",
    "def countNull(df,var):\n",
    "    return df.where(df[var].isNull()).count()\n",
    "\n",
    "def countEmptyString(df,var):\n",
    "    return df[df[var].isin(\"\")].count()\n",
    "\n",
    "def countZero(df,var):\n",
    "    return df[df[var].isin(0)].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "missing = {var: countNull(df,var) for var in df.columns}\n",
    "missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "missing = {var: countEmptyString(df, var) for var in df.columns}\n",
    "missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "missing = {var: countZero(df, var) for var in df.columns}\n",
    "missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "age_mean = df.groupBy().mean('Age').first()\n",
    "age_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "age_mean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "age_mean = df.groupBy().mean('Age').first()[0]\n",
    "fare_mean = df.groupBy().mean('Fare').first()[0]\n",
    "age_mean, fare_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import count, col \n",
    "def mode_spark(df, column):\n",
    "    # Group by column and count the number of occurrences\n",
    "    # of each x value\n",
    "    counts = df.groupBy(column).count()\n",
    "\n",
    "    # - Find the maximum value in the 'counts' column\n",
    "    # - Join with the counts dataframe to select the row\n",
    "    #   with the maximum count\n",
    "    # - Select the first element of this dataframe and\n",
    "    #   take the value in column\n",
    "    mode = counts.join(\n",
    "        counts.agg(F.max('count').alias('count')),\n",
    "        on='count'\n",
    "    ).limit(1).select(column)\n",
    "\n",
    "    return mode.first()[column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Embarked_mode = mode_spark(df, 'Embarked')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.na.fill({'Age':age_mean,'Fare':fare_mean, 'Embarked':Embarked_mode})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is wrong with what I just did?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Univariate\n",
    "    - Winsorization\n",
    "* Multivariate\n",
    "\n",
    "* Is it a good idea?\n",
    "* Know your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Domain Expertise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    " \n",
    "## create user defined function to extract title\n",
    "getTitle = udf(lambda name: name.split('.')[0].strip(), StringType())\n",
    "df = df.withColumn('Title', getTitle(df['Name']))\n",
    " \n",
    "df.select('Name','Title').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "getTitle = udf(lambda name: name.split('.')[0].split(',')[1].strip(), StringType())\n",
    "df = df.withColumn('Title', getTitle(df['Name']))\n",
    " \n",
    "df.select('Name','Title').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Variable treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some algorithms can handle categorical variables directly, some can't.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Label Encoder\n",
    "    - It is used to transform non-numerical labels to numerical labels (or nominal categorical variables)\n",
    "    - Numerical labels are always between 0 and n_classes-1\n",
    "    - May introduce spurious relationship\n",
    "        * Age and City\n",
    "* One Hot Encoding\n",
    "    - Encodes categorical integer features using a one-hot aka one-of-K scheme\n",
    "    - Preferable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Encoding (Indexing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "catVars = ['Pclass','Sex','Embarked','Title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " \n",
    "## index Sex variable\n",
    "si = StringIndexer(inputCol = 'Pclass', outputCol = 'Pclass_indexed')\n",
    "df_indexed = si.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "si = StringIndexer(inputCol = 'Sex', outputCol = 'Sex_indexed')\n",
    "df_indexed = si.fit(df_indexed).transform(df_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "si = StringIndexer(inputCol = 'Embarked', outputCol = 'Embarked_indexed')\n",
    "df_indexed = si.fit(df_indexed).transform(df_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "si = StringIndexer(inputCol = 'Title', outputCol = 'Title_indexed')\n",
    "df_indexed = si.fit(df_indexed).transform(df_indexed)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(df) for column in catVars ]\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "df_indexed = pipeline.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_indexed.select('Embarked','Embarked_indexed').show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The categorical features are indexed in resulting data\n",
    "* Embarked is mapped S=>0, C=>1, Q=>2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### StringIndexer\n",
    "\n",
    "* Maps a string column of labels to a column of label indices\n",
    "* If the input column is numeric, we cast it to string and index the string values\n",
    "* The indices are in [0, numLabels), ordered by label frequencies\n",
    "    - So the most frequent label gets index 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer\n",
    "\n",
    "* transform one dataset into another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimator\n",
    "\n",
    "* fit models to data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipelines\n",
    "\n",
    " \n",
    "* A Pipeline consists of a sequence of stages, each of which is either an Estimator or a Transformer\n",
    "* When Pipeline.fit() is called, the stages are executed in order\n",
    "    - If a stage is an Estimator, its Estimator.fit() method will be called on the input dataset to fit a model\n",
    "        * Then the model, which is a transformer, will be used to transform the dataset as the input to the next stage\n",
    "    - If a stage is a Transformer, its Transformer.transform() method will be called to produce the dataset for the next stage\n",
    "* The fitted model from a Pipeline is a PipelineModel, which consists of fitted models and transformers, corresponding to the pipeline stages\n",
    "* If there are no stages, the pipeline acts as an identity transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timeseries Variable treatments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Shattering\n",
    "* No time/day variables here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data format translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this step, we get the data in the format or data type expected by the algorithms\n",
    "* In the case of Spark MLlib, this includes \n",
    "    - local vector\n",
    "    - dense or sparse vectors\n",
    "    - labeled points\n",
    "    - local matrix\n",
    "    - distributed matrix with row matrix\n",
    "    - indexed row matrix\n",
    "    - coordinate matrix\n",
    "    - block matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, we need convert features to Vectors (either SparseVector or DenseVector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import DenseVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "catVarsIndexed = [i + '_indexed' for i in catVars]\n",
    "catVarsIndexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featuresCol = numVars + catVarsIndexed\n",
    "featuresCol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featuresCol.remove('Survived')\n",
    "featuresCol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labelCol = ['Mark','Survived']\n",
    "labelCol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "row = Row('mark','label','features') \n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_indexed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_indexed = df_indexed[labelCol + featuresCol]\n",
    "df_indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 0-mark, 1-label, 2-features\n",
    "# map features to DenseVector\n",
    "lf = df_indexed.rdd.map(lambda r: (row(r[0], r[1],DenseVector(r[2:])))).toDF()\n",
    "lf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# index label\n",
    "# convert numeric label to categorical, which is required by\n",
    "# decisionTree and randomForest\n",
    "lf = StringIndexer(inputCol = 'label', outputCol='index').fit(lf).transform(lf)\n",
    " \n",
    "lf.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split back into train/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = lf.where(lf.mark =='train')\n",
    "test = lf.where(lf.mark =='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random split further to get train/validate\n",
    "train, validate = train.randomSplit([0.7,0.3], seed =121)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Train Data Number of Row: '+ str(train.count()))\n",
    "print('Validate Data Number of Row: '+ str(validate.count()))\n",
    "print('Test Data Number of Row: '+ str(test.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ML is built based on DataFrame, while mllib is based on RDD\n",
    "* We'll fit the logistic, decision tree and random forest models from ML packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    " \n",
    "# regPara: lasso regularisation parameter (L1)\n",
    "lr = LogisticRegression(maxIter = 100, regParam = 0.05, labelCol='index').fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate model based on auc ROC(default for binary classification)\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    " \n",
    "def testModel(model, validate = validate):\n",
    "    pred = model.transform(validate)\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol = 'index')\n",
    "    return evaluator.evaluate(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('AUC ROC of Logistic Regression model is: ' + str(testModel(lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('AUC ROC of Logistic Regression model is: ' + str(testModel(lr, validate=test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_test = lr.transform(test)\n",
    "pred_test.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier, GBTClassifier\n",
    " \n",
    "dt = DecisionTreeClassifier(maxDepth = 3, labelCol ='index').fit(train)\n",
    "rf = RandomForestClassifier(numTrees = 100, labelCol = 'index').fit(train)\n",
    "gbt = GBTClassifier(maxIter = 10, labelCol = 'index').fit(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models = {'LogisticRegression':lr,\n",
    "          'DecistionTree':dt,\n",
    "          'RandomForest':rf}\n",
    " \n",
    "modelPerf = {k:testModel(v) for k,v in models.items()}\n",
    "print(modelPerf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def model_acc(model, validate=validate):\n",
    "    pred = model.transform(validate)\n",
    "    eval_vec = np.array(get_column(pred, \"label\")) == np.array(get_column(pred, \"prediction\")) \n",
    "    return(eval_vec.sum()/len(eval_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_acc(gbt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models = {'LogisticRegression':lr,\n",
    "          'DecistionTree':dt,\n",
    "          'RandomForest':rf,\n",
    "          'GradientBoostingMachines':gbt}\n",
    "\n",
    "modelPerf = {k:model_acc(v) for k,v in models.items()}\n",
    "print(modelPerf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    dt = DecisionTreeClassifier(maxDepth = i, labelCol ='index').fit(train)\n",
    "    print('AUC ROC of Decision Tree model is' + '(for maxDepth= ' + str(i) + '): ' + str(testModel(dt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(5, 200):\n",
    "    rf = RandomForestClassifier(numTrees = i, labelCol = 'index').fit(train)\n",
    "    print('AUC ROC of Random Forest model is' + '(for numTrees= ' + str(i) + '): ' + str(testModel(rf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorframes as tfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfs.print_schema(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x = tfs.analyze(train.select(\"features\"))\n",
    "train_x.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_y = tfs.analyze(train.select(\"label\"))\n",
    "train_y.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib import learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Define input values\n",
    "x = tf.placeholder(shape=[None,8],dtype=tf.float32, name='x-input')\n",
    "y_ = tf.placeholder(shape=[None,1],dtype=tf.float32, name='y-input')\n",
    "\n",
    "#lets normalize all features along the columns\n",
    "x_n = tf.nn.l2_normalize(x,1)\n",
    "\n",
    "print('Input placeholders created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x.consumers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Define Weights and Bias\n",
    "W = tf.Variable(tf.zeros(shape=[8,1]), name=\"Weights\")\n",
    "b = tf.Variable(tf.zeros(shape=[1]),name=\"Bias\")\n",
    "print('Weight and Bias created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W.value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Price prediction\n",
    "y = tf.add(tf.matmul(x_n,W),b,name='output')\n",
    "\n",
    "#Loss\n",
    "loss = tf.reduce_mean(tf.square(y-y_),name='Loss')\n",
    "\n",
    "print('Output and loss Ops created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Lets define Gradient Descent Optimizer\n",
    "train_op = tf.train.GradientDescentOptimizer(0.03).minimize(loss)\n",
    "\n",
    "print('Optimizer is created. Graph building is completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Lets start graph Execution\n",
    "with tf.Session() as sess:\n",
    "    # variables need to be initialized before we can use them\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    #lets train\n",
    "    training_epochs = 1000  #how many times data need to be shown to model\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        \n",
    "        #Calculate train_op and loss\n",
    "        train_model, train_loss = sess.run([train_op,loss],feed_dict={x:train_x, y_:train_y})\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print ('Training loss at step: ', epoch, ' is ', train_loss)\n",
    "    print (sess.run([W,b]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of the K-Means algorithm, while distributing the computations on a cluster.\n",
    "Given a set of feature vectors, this algorithm runs the K-Means clustering algorithm starting\n",
    "from a given set of centroids.\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorframes as tfs\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf_compute_distances(points, start_centers):\n",
    "    \"\"\"\n",
    "    Given a set of points and some centroids, computes the distance from each point to each\n",
    "    centroid.\n",
    "    :param points: a 2d TF tensor of shape num_points x dim\n",
    "    :param start_centers: a numpy array of shape num_centroid x dim\n",
    "    :return: a TF tensor of shape num_points x num_centroids\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(\"distances\"):\n",
    "        # The dimensions in the problem\n",
    "        (num_centroids, _) = np.shape(start_centers)\n",
    "        # The shape of the block is extracted as a TF variable.\n",
    "        num_points = tf.shape(points)[0]\n",
    "        # The centers are embedded in the TF program.\n",
    "        centers = tf.constant(start_centers)\n",
    "        # Computation of the minimum distance. This is a standard implementation that follows\n",
    "        # what MLlib does.\n",
    "        squares = tf.reduce_sum(tf.square(points), reduction_indices=1)\n",
    "        center_squares = tf.reduce_sum(tf.square(centers), reduction_indices=1)\n",
    "        prods = tf.matmul(points, centers, transpose_b = True)\n",
    "        # This code simply expresses two outer products: center_squares * ones(num_points)\n",
    "        # and ones(num_centroids) * squares\n",
    "        t1a = tf.expand_dims(center_squares, 0)\n",
    "        t1b = tf.stack([num_points, 1])\n",
    "        t1 = tf.tile(t1a, t1b)\n",
    "        t2a = tf.expand_dims(squares, 1)\n",
    "        t2b = tf.stack([1, num_centroids])\n",
    "        t2 = tf.tile(t2a, t2b)\n",
    "        distances = t1 + t2 - 2 * prods\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_one_step(dataframe, start_centers):\n",
    "    \"\"\"\n",
    "    Performs one iteration of K-Means.\n",
    "    This function takes a dataframe with dense feature vectors, a set of centroids, and returns\n",
    "    a new set of centroids along with the total distance of points to centroids.\n",
    "    This function calculates for each point the closest centroid and then aggregates the newly\n",
    "    formed clusters to find the new centroids.\n",
    "    This function uses Spark to distribute the aggregation amongst the node.\n",
    "    :param dataframe: a dataframe containing a column of features (an array of doubles)\n",
    "    :param start_centers: a k x m matrix with k the number of centroids and m the number of features\n",
    "    :return: a k x m matrix, and a positive double\n",
    "    \"\"\"\n",
    "    # The dimensions in the problem\n",
    "    (num_centroids, num_features) = np.shape(start_centers)\n",
    "    # For each feature vector, compute the nearest centroid and the distance to that centroid.\n",
    "    # The index of the nearest centroid is stored in the 'indexes' column.\n",
    "    # We also add a column of 1's that will be reduced later to count the number of elements in\n",
    "    # each cluster.\n",
    "    with tf.Graph().as_default() as g:\n",
    "        # The placeholder for the input: we use the block format\n",
    "        points = tf.placeholder(tf.double, shape=[None, num_features], name='features')\n",
    "        # The shape of the block is extracted as a TF variable.\n",
    "        num_points = tf.stack([tf.shape(points)[0]], name=\"num_points\")\n",
    "        distances = tf_compute_distances(points, start_centers)\n",
    "        # The outputs of the program.\n",
    "        # The closest centroids are extracted.\n",
    "        indexes = tf.argmin(distances, 1, name='indexes')\n",
    "        # This could be done based on the indexes as well.\n",
    "        min_distances = tf.reduce_min(distances, 1, name='min_distances')\n",
    "        counts = tf.tile(tf.constant([1]), num_points, name='count')\n",
    "        df2 = tfs.map_blocks([indexes, counts, min_distances], dataframe)\n",
    "    # Perform the reduction: we regroup the points by their centroid indexes.\n",
    "    gb = df2.groupBy(\"indexes\")\n",
    "    with tf.Graph().as_default() as g:\n",
    "        # Look at the documentation of tfs.aggregate for the naming conventions of the placeholders.\n",
    "        x_input = tfs.block(df2, \"features\", tf_name=\"features_input\")\n",
    "        count_input = tfs.block(df2, \"count\", tf_name=\"count_input\")\n",
    "        md_input = tfs.block(df2, \"min_distances\", tf_name=\"min_distances_input\")\n",
    "        # Each operation is just the sum.\n",
    "        x = tf.reduce_sum(x_input, [0], name='features')\n",
    "        count = tf.reduce_sum(count_input, [0], name='count')\n",
    "        min_distances = tf.reduce_sum(md_input, [0], name='min_distances')\n",
    "        df3 = tfs.aggregate([x, count, min_distances], gb)\n",
    "    # Get the new centroids\n",
    "    df3_c = df3.collect()\n",
    "    # The new centroids.\n",
    "    new_centers = np.array([np.array(row.features) / row['count'] for row in df3_c])\n",
    "    total_distances = np.sum([row['min_distances'] for row in df3_c])\n",
    "    return (new_centers, total_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_one_step2(dataframe, start_centers):\n",
    "    \"\"\"\n",
    "    Performs one iteration of K-Means.\n",
    "    This function takes a dataframe with dense feature vectors, a set of centroids, and returns\n",
    "    a new set of centroids along with the total distance of points to centroids.\n",
    "    This function calculates for each point the closest centroid and then aggregates the newly\n",
    "    formed clusters to find the new centroids.\n",
    "    This function performs most of the aggregation in TensorFlow.\n",
    "    :param dataframe: a dataframe containing a column of features (an array of doubles)\n",
    "    :param start_centers: a k x m matrix with k the number of centroids and m the number of features\n",
    "    :return: a k x m matrix, and a positive double\n",
    "    \"\"\"\n",
    "    # The dimensions in the problem\n",
    "    (num_centroids, _) = np.shape(start_centers)\n",
    "    # For each feature vector, compute the nearest centroid and the distance to that centroid.\n",
    "    # The index of the nearest centroid is stored in the 'indexes' column.\n",
    "    # We also add a column of 1's that will be reduced later to count the number of elements in\n",
    "    # each cluster.\n",
    "    with tf.Graph().as_default() as g:\n",
    "        # The placeholder for the input: we use the block format\n",
    "        points = tf.placeholder(tf.double, shape=[None, num_features], name='features')\n",
    "        # The distances\n",
    "        distances = tf_compute_distances(points, start_centers)\n",
    "        # The rest of this block performs a pre-aggregation step in TF, to limit the\n",
    "        # communication between TF and Spark.\n",
    "        # The closest centroids are extracted.\n",
    "        indexes = tf.argmin(distances, 1, name='indexes')\n",
    "        min_distances = tf.reduce_min(distances, 1, name='min_distances')\n",
    "        num_points = tf.stack([tf.shape(points)[0]], name=\"num_points\")\n",
    "        counts = tf.tile(tf.constant([1]), num_points, name='count')\n",
    "        # These compute the aggregate based on the indexes.\n",
    "        block_points = tf.unsorted_segment_sum(points, indexes, num_centroids, name=\"block_points\")\n",
    "        block_counts = tf.unsorted_segment_sum(counts, indexes, num_centroids, name=\"block_counts\")\n",
    "        block_distances = tf.reduce_sum(min_distances, name=\"block_distances\")\n",
    "        # One leading dimension is added to express the fact that the previous elements are just\n",
    "        # one row in the final dataframe.\n",
    "        # The final dataframe has one row per block.\n",
    "        agg_points = tf.expand_dims(block_points, 0, name=\"agg_points\")\n",
    "        agg_counts = tf.expand_dims(block_counts, 0, name=\"agg_counts\")\n",
    "        agg_distances = tf.expand_dims(block_distances, 0, name=\"agg_distances\")\n",
    "        # Using trimming to drop the original data (we are just returning one row of data per\n",
    "        # block).\n",
    "        df2 = tfs.map_blocks([agg_points, agg_counts, agg_distances],\n",
    "                             dataframe, trim=True)\n",
    "    # Now we simply collect and sum the elements\n",
    "    with tf.Graph().as_default() as g:\n",
    "        # Look at the documentation of tfs.aggregate for the naming conventions of the placeholders.\n",
    "        x_input = tf.placeholder(tf.double,\n",
    "                                 shape=[None, num_centroids, num_features],\n",
    "                                 name='agg_points_input')\n",
    "        count_input = tf.placeholder(tf.int32,\n",
    "                                     shape=[None, num_centroids],\n",
    "                                     name='agg_counts_input')\n",
    "        md_input = tf.placeholder(tf.double,\n",
    "                                  shape=[None],\n",
    "                                  name='agg_distances_input')\n",
    "        # Each operation is just the sum.\n",
    "        x = tf.reduce_sum(x_input, [0], name='agg_points')\n",
    "        count = tf.reduce_sum(count_input, [0], name='agg_counts')\n",
    "        min_distances = tf.reduce_sum(md_input, [0], name='agg_distances')\n",
    "        (x_, count_, total_distances) = tfs.reduce_blocks([x, count, min_distances], df2)\n",
    "    # The new centers\n",
    "    new_centers = (x_.T / (count_ + 1e-7)).T\n",
    "    return (new_centers, total_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kmeanstf(dataframe, init_centers, num_iters = 5, tf_aggregate = True):\n",
    "    \"\"\"\n",
    "    Runs the K-Means algorithm on a set of feature points.\n",
    "    This function takes a dataframe with dense feature vectors, a set of centroids, and returns\n",
    "    a new set of centroids along with the total distance of points to centroids.\n",
    "    :param dataframe: a dataframe containing a column of features (an array of doubles)\n",
    "    :param init_centers: the centers to start from\n",
    "    :param num_iters:  the maximum number of iterations to run\n",
    "    :return: a k x m matrix, and a list of positive doubles\n",
    "    \"\"\"\n",
    "    step_fun = run_one_step2 if tf_aggregate else run_one_step\n",
    "    c = init_centers\n",
    "    d = np.Inf\n",
    "    ds = []\n",
    "    for i in range(num_iters):\n",
    "        (c1, d1) = step_fun(dataframe, c)\n",
    "        print(\"Step =\", i, \", overall distance = \", d1)\n",
    "        c = c1\n",
    "        if d == d1:\n",
    "            break\n",
    "        d = d1\n",
    "        ds.append(d1)\n",
    "    return c, ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here is a an example of usage:\n",
    "try:\n",
    "    sc.setLogLevel('INFO')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans, KMeansModel\n",
    "from pyspark.ml.linalg import VectorUDT, _convert_to_vector\n",
    "from pyspark.sql.types import Row, StructField, StructType\n",
    "import time\n",
    "\n",
    "# Small vectors\n",
    "num_features = 100\n",
    "# The number of clusters\n",
    "k = 10\n",
    "num_points = 10000\n",
    "num_iters = 10\n",
    "FEATURES_COL = \"features\"\n",
    "\n",
    "np.random.seed(2)\n",
    "np_data = [x.tolist() for x in np.random.uniform(0.0, 1.0, size=(num_points, num_features))]\n",
    "schema = StructType([StructField(FEATURES_COL, VectorUDT(), False)])\n",
    "mllib_rows = [Row(_convert_to_vector(x)) for x in np_data]\n",
    "mllib_df = spark.createDataFrame(mllib_rows, schema).coalesce(1).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([[r] for r in np_data]).toDF(FEATURES_COL).coalesce(1)\n",
    "# For now, analysis is still required. We cache the output because we are going to perform\n",
    "# multiple runs on the dataset.\n",
    "df0 = tfs.analyze(df).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mllib_df.count()\n",
    "df0.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "init_centers = np.random.randn(k, num_features)\n",
    "start_centers = init_centers\n",
    "dataframe = df0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ta_0 = time.time()\n",
    "kmeans = KMeans().setK(k).setSeed(1).setFeaturesCol(FEATURES_COL).setInitMode(\n",
    "        \"random\").setMaxIter(num_iters)\n",
    "mod = kmeans.fit(mllib_df)\n",
    "ta_1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step = 0 , overall distance =  1145400.75877\n",
      "Step = 1 , overall distance =  82752.0755883\n",
      "Step = 2 , overall distance =  82471.0331805\n",
      "Step = 3 , overall distance =  82334.6093698\n",
      "Step = 4 , overall distance =  82278.9850022\n",
      "Step = 5 , overall distance =  82252.0602253\n",
      "Step = 6 , overall distance =  82235.9084488\n",
      "Step = 7 , overall distance =  82225.1000004\n",
      "Step = 8 , overall distance =  82216.9000307\n",
      "Step = 9 , overall distance =  82209.822798\n"
     ]
    }
   ],
   "source": [
    "tb_0 = time.time()\n",
    "(centers, agg_distances) = kmeanstf(df0, init_centers, num_iters=num_iters, tf_aggregate=False)\n",
    "tb_1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step = 0 , overall distance =  1145400.75877\n",
      "Step = 1 , overall distance =  82752.0755883\n",
      "Step = 2 , overall distance =  82471.0331805\n",
      "Step = 3 , overall distance =  82334.6093698\n",
      "Step = 4 , overall distance =  82278.9850022\n",
      "Step = 5 , overall distance =  82252.0602253\n",
      "Step = 6 , overall distance =  82235.9084488\n",
      "Step = 7 , overall distance =  82225.1000004\n",
      "Step = 8 , overall distance =  82216.9000307\n",
      "Step = 9 , overall distance =  82209.822798\n"
     ]
    }
   ],
   "source": [
    "tc_0 = time.time()\n",
    "(centers, agg_distances) = kmeanstf(df0, init_centers, num_iters=num_iters, tf_aggregate=True)\n",
    "tc_1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mllib: 1.4775249958 tf+spark: 67.9400539398 tf: 2.10477209091\n"
     ]
    }
   ],
   "source": [
    "mllib_dt = ta_1 - ta_0\n",
    "tf_dt = tb_1 - tb_0\n",
    "tf2_dt = tc_1 - tc_0\n",
    "\n",
    "print(\"mllib:\", mllib_dt, \"tf+spark:\",tf_dt, \"tf:\",tf2_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
