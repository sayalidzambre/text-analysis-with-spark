{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named trainer.model",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-17fd174d81a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0miris\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named trainer.model"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import os\n",
    "import subprocess\n",
    "import uuid\n",
    "import sys\n",
    "\n",
    "import apache_beam as beam\n",
    "import trainer.model as iris\n",
    "import trainer.task as task\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import google.cloud.ml as ml\n",
    "import google.cloud.ml.io as io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model variables\n",
    "MODEL_NAME = 'iris'\n",
    "TRAINER_NAME = 'trainer-1.0.tar.gz'\n",
    "METADATA_FILE_NAME = 'metadata.json'\n",
    "MODULE_NAME = 'trainer.task'\n",
    "EXPORT_SUBDIRECTORY = 'model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _default_project():\n",
    "  get_project = ['gcloud', 'config', 'list', 'project',\n",
    "                 '--format=value(core.project)']\n",
    "\n",
    "  with open(os.devnull, 'w') as dev_null:\n",
    "    return subprocess.check_output(get_project, stderr=dev_null).strip()\n",
    "\n",
    "def parse_arguments(argv):\n",
    "  parser = argparse.ArgumentParser(\n",
    "      description='Runs Training on the Iris model data.')\n",
    "  parser.add_argument(\n",
    "      '--project_id', help='The project to which the job will be submitted.')\n",
    "  parser.add_argument(\n",
    "      '--cloud', action='store_true', help='Run preprocessing on the cloud.')\n",
    "  parser.add_argument(\n",
    "      '--training_data',\n",
    "      default='gs://cloud-ml-data/iris/data_train.csv',\n",
    "      help='Data to analyze and encode as training features.')\n",
    "  parser.add_argument(\n",
    "      '--eval_data',\n",
    "      default='gs://cloud-ml-data/iris/data_eval.csv',\n",
    "      help='Data to encode as evaluation features.')\n",
    "  parser.add_argument(\n",
    "      '--predict_data',\n",
    "      default='gs://cloud-ml-data/iris/data_predict.csv',\n",
    "      help='Data to encode as prediction features.')\n",
    "  parser.add_argument(\n",
    "      '--output_dir',\n",
    "      default=None,\n",
    "      help=('Google Cloud Storage or Local directory in which '\n",
    "            'to place outputs.'))\n",
    "  parser.add_argument(\n",
    "      '--deploy_model_name',\n",
    "      default='iris',\n",
    "      help=('If --cloud is used, the model is deployed with this '\n",
    "            'name. The default is iris.'))\n",
    "  parser.add_argument(\n",
    "      '--deploy_model_version',\n",
    "      default='v' + uuid.uuid4().hex[:4],\n",
    "      help=('If --cloud is used, the model is deployed with this '\n",
    "            'version. The default is four random characters.'))\n",
    "  parser.add_argument(\n",
    "      '--sdk_location',\n",
    "      default=None,\n",
    "      help=('Specify the location of the Dataflow SDK. If not specified the '\n",
    "            'SDK will do the right thing.'))\n",
    "  parser.add_argument(\n",
    "      '--endpoint',\n",
    "      default=None,\n",
    "      help=('HTTPS endpoint to run training against. If not specified the '\n",
    "            'SDK will do the right thing.'))\n",
    "  args, passthrough_args = parser.parse_known_args(args=argv[1:])\n",
    "\n",
    "  args.trainer_job_args = passthrough_args\n",
    "\n",
    "  if not args.project_id:\n",
    "    args.project_id = _default_project()\n",
    "\n",
    "  if not args.output_dir:\n",
    "    if args.cloud:\n",
    "      args.output_dir = os.path.join('gs://' + args.project_id + '-ml',\n",
    "                                     MODEL_NAME)\n",
    "    else:\n",
    "      path = 'output'\n",
    "      if not os.path.isdir(path):\n",
    "        os.makedirs(path)\n",
    "      args.output_dir = path\n",
    "\n",
    "  args.trainer_uri = os.path.join(args.output_dir, TRAINER_NAME)\n",
    "\n",
    "  return args\n",
    "\n",
    "\n",
    "def preprocess(pipeline, training_data, eval_data, predict_data, output_dir):\n",
    "  \"\"\"Read in input files, runs ml.Preprocess, and writes preprocessed output.\n",
    "  Args:\n",
    "    pipeline: beam pipeline\n",
    "    training_data, eval_data, predict_data: file paths to input csv files.\n",
    "    output_dir: file path to where to write all the output files.\n",
    "  Returns:\n",
    "    metadata and preprocessed features as pcollections.\n",
    "  \"\"\"\n",
    "  feature_set = iris.IrisFeatures()\n",
    "\n",
    "  coder_with_target = io.CsvCoder.from_feature_set(feature_set,\n",
    "                                                   feature_set.csv_columns)\n",
    "  coder_without_target = io.CsvCoder.from_feature_set(feature_set,\n",
    "                                                      feature_set.csv_columns,\n",
    "                                                      has_target_columns=False)\n",
    "\n",
    "  train = (\n",
    "      pipeline\n",
    "      | 'ReadTrainingData'\n",
    "      >> beam.io.textio.ReadFromText(training_data, coder=coder_with_target))\n",
    "  evaluate = (\n",
    "      pipeline\n",
    "      | 'ReadEvalData'\n",
    "      >> beam.io.textio.ReadFromText(eval_data, coder=coder_with_target))\n",
    "  predict = (\n",
    "      pipeline\n",
    "      | 'ReadPredictData'\n",
    "      >> beam.io.textio.ReadFromText(predict_data, coder=coder_without_target))\n",
    "\n",
    "  # TODO(b/32726166) Update input_format and format_metadata to read from these\n",
    "  # values directly from the coder.\n",
    "  (metadata, train_features, eval_features, predict_features) = (\n",
    "      (train, evaluate, predict)\n",
    "      | 'Preprocess' >> ml.Preprocess(\n",
    "          feature_set,\n",
    "          input_format='csv',\n",
    "          format_metadata={\n",
    "              'headers': feature_set.csv_columns\n",
    "          }))\n",
    "\n",
    "  # Writes metadata.json - specified through METADATA_FILENAME- (text file),\n",
    "  # features_train, features_eval, and features_eval (TFRecord files)\n",
    "  (metadata | 'SaveMetadata'\n",
    "   >> io.SaveMetadata(os.path.join(output_dir, METADATA_FILE_NAME)))\n",
    "\n",
    "  # We turn off sharding of the feature files because the dataset is very small.\n",
    "  (train_features | 'SaveTrain'\n",
    "   >> io.SaveFeatures(os.path.join(output_dir, 'features_train')))\n",
    "  (eval_features | 'SaveEval'\n",
    "   >> io.SaveFeatures(os.path.join(output_dir, 'features_eval')))\n",
    "  (predict_features | 'SavePredict'\n",
    "   >> io.SaveFeatures(os.path.join(output_dir, 'features_predict')))\n",
    "\n",
    "  return metadata, train_features, eval_features, predict_features\n",
    "\n",
    "\n",
    "def get_train_parameters(trainer_uri, endpoint, metadata, trainer_job_args):\n",
    "  return {\n",
    "      'package_uris': [trainer_uri, ml.version.installed_sdk_location],\n",
    "      'python_module': MODULE_NAME,\n",
    "      'export_subdir': EXPORT_SUBDIRECTORY,\n",
    "      'cloud_ml_endpoint': endpoint,\n",
    "      'metadata': metadata,\n",
    "      'label': 'Train',\n",
    "      'region': 'us-central1',\n",
    "      'scale_tier': 'STANDARD_1',\n",
    "      'job_args': trainer_job_args\n",
    "  }\n",
    "\n",
    "\n",
    "def train(pipeline, output_dir, train_args_dict,\n",
    "          train_features=None, eval_features=None, metadata=None):\n",
    "  if not train_features:\n",
    "    train_features = (\n",
    "        pipeline\n",
    "        | 'ReadTrain'\n",
    "        >> io.LoadFeatures(os.path.join(output_dir, 'features_train*')))\n",
    "  if not eval_features:\n",
    "    eval_features = (\n",
    "        pipeline\n",
    "        | 'ReadEval'\n",
    "        >> io.LoadFeatures(os.path.join(output_dir, 'features_eval*')))\n",
    "  if not metadata:\n",
    "    metadata = (\n",
    "        pipeline\n",
    "        | 'ReadMetadata'\n",
    "        >> io.LoadMetadata(os.path.join(output_dir, METADATA_FILE_NAME)))\n",
    "\n",
    "  trained_model, results = (\n",
    "      (train_features, eval_features)\n",
    "      | 'Train'\n",
    "      >> ml.Train(**train_args_dict))\n",
    "\n",
    "  trained_model | 'SaveModel' >> io.SaveModel(\n",
    "      os.path.join(output_dir, 'saved_model'))\n",
    "  results | io.SaveTrainingJobResult(\n",
    "      os.path.join(output_dir, 'train_results'))\n",
    "\n",
    "  return trained_model, results\n",
    "\n",
    "\n",
    "def evaluate(pipeline, output_dir, trained_model=None, eval_features=None):\n",
    "  if not eval_features:\n",
    "    eval_features = (\n",
    "        pipeline\n",
    "        | 'ReadEval'\n",
    "        >> io.LoadFeatures(os.path.join(output_dir, 'features_eval*')))\n",
    "  if not trained_model:\n",
    "    trained_model = (pipeline\n",
    "                     | 'LoadModel' >>\n",
    "                     io.LoadModel(os.path.join(output_dir, 'saved_model')))\n",
    "\n",
    "  # Run our evaluation data through a Batch Evaluation, then pull out just\n",
    "  # the expected and predicted target values.\n",
    "  vocab_loader = LazyVocabLoader(os.path.join(output_dir, METADATA_FILE_NAME))\n",
    "\n",
    "  evaluations = (eval_features\n",
    "                 | 'Evaluate' >> ml.Evaluate(trained_model)\n",
    "                 | 'CreateEvaluations' >> beam.Map(\n",
    "                     make_evaluation_dict, vocab_loader))\n",
    "  coder = io.CsvCoder(\n",
    "      column_names=['key', 'target', 'predicted', 'score', 'target_label',\n",
    "                    'predicted_label', 'all_scores'],\n",
    "      numeric_column_names=['target', 'predicted', 'score'])\n",
    "  (evaluations\n",
    "   | 'WriteEvaluation'\n",
    "   >> beam.io.textio.WriteToText(os.path.join(output_dir,\n",
    "                                              'model_evaluations'),\n",
    "                                 file_name_suffix='.csv',\n",
    "                                 coder=coder))\n",
    "  return evaluations\n",
    "\n",
    "\n",
    "class LazyVocabLoader(object):\n",
    "  \"\"\"Lazy load the vocabulary when needed on the worker.\"\"\"\n",
    "\n",
    "  def __init__(self, metadata_path):\n",
    "    self.metadata_path = metadata_path\n",
    "    self.vocab = {}  # dict of strings to numbers\n",
    "    self.reverse_vocab = []  # list of strings.\n",
    "\n",
    "  def get_vocab(self):\n",
    "    # Returns a dictionary of Iris labels to consecutive integer identifiers.\n",
    "    if not self.vocab:\n",
    "      metadata = ml.features.FeatureMetadata.get_metadata(self.metadata_path)\n",
    "      self.vocab = metadata.columns['species']['vocab']\n",
    "    return self.vocab\n",
    "\n",
    "  def get_reverse_vocab(self):\n",
    "    # Returns a list of consecutive integer identifiers to Iris labels.\n",
    "    if not self.reverse_vocab:\n",
    "      vocab = self.get_vocab()\n",
    "      self.reverse_vocab = [None] * len(vocab)\n",
    "      for species_name, index_number in vocab.iteritems():\n",
    "        self.reverse_vocab[index_number] = species_name\n",
    "    return self.reverse_vocab\n",
    "\n",
    "\n",
    "def make_evaluation_dict((input_dict, output_dict), vocab_loader):\n",
    "  \"\"\"Make summary dict for evaluation.\n",
    "  Must contain the schema \"target, predicted, score[optional]\" for use with\n",
    "  ml.AnalyzeModel.\n",
    "  Args:\n",
    "    input_dict: Input to the TF model ({'input_example:0': tf.Example string})\n",
    "    output_dict: output of the TF model ({'key': ?, 'score': ?, 'label': ?})\n",
    "    vocab_loader: loads the species vocab.\n",
    "  Returns:\n",
    "    A dict suitable for ml.AnalyzeModel that contains other summary data.\n",
    "  \"\"\"\n",
    "  vocab = vocab_loader.get_vocab()\n",
    "  reverse_vocab = vocab_loader.get_reverse_vocab()\n",
    "\n",
    "  scores = output_dict[task.SCORES_OUTPUT_COLUMN]\n",
    "  predicted_label = output_dict[task.LABEL_OUTPUT_COLUMN]\n",
    "\n",
    "  ex = tf.train.Example()\n",
    "  ex.ParseFromString(input_dict.values()[0])\n",
    "  target = ex.features.feature[task.TARGET_FEATURE_COLUMN].int64_list.value[0]\n",
    "\n",
    "  return {\n",
    "      'key': output_dict[task.KEY_OUTPUT_COLUMN],\n",
    "      'target': target,\n",
    "      'predicted': vocab[predicted_label],\n",
    "      'score': max(scores),\n",
    "      'all_scores': scores,\n",
    "      'target_label': reverse_vocab[target],\n",
    "      'predicted_label': predicted_label,\n",
    "  }\n",
    "\n",
    "\n",
    "def deploy_model(pipeline, output_dir, endpoint, model_name, version_name,\n",
    "                 trained_model=None):\n",
    "  if not trained_model:\n",
    "    trained_model = (pipeline\n",
    "                     | 'LoadModel' >>\n",
    "                     io.LoadModel(os.path.join(output_dir, 'saved_model')))\n",
    "\n",
    "  return trained_model | ml.DeployVersion(model_name, version_name, endpoint)\n",
    "\n",
    "\n",
    "def model_analysis(pipeline, output_dir, evaluation_data=None, metadata=None):\n",
    "  if not metadata:\n",
    "    metadata = (\n",
    "        pipeline\n",
    "        | 'LoadMetadataForAnalysis'\n",
    "        >> io.LoadMetadata(os.path.join(output_dir, METADATA_FILE_NAME)))\n",
    "  if not evaluation_data:\n",
    "    coder = io.CsvCoder(\n",
    "        column_names=['key', 'target', 'predicted', 'score', 'target_label',\n",
    "                      'predicted_label', 'all_scores'],\n",
    "        numeric_column_names=['target', 'predicted', 'score'])\n",
    "    evaluation_data = (\n",
    "        pipeline\n",
    "        | 'ReadEvaluation'\n",
    "        >> beam.io.ReadFromText(os.path.join(output_dir,\n",
    "                                             'model_evaluations*'),\n",
    "                                coder=coder))\n",
    "  confusion_matrix, precision_recall, logloss = (evaluation_data\n",
    "                                                 | 'AnalyzeModel'\n",
    "                                                 >> ml.AnalyzeModel(metadata))\n",
    "\n",
    "  confusion_matrix | io.SaveConfusionMatrixCsv(\n",
    "      os.path.join(output_dir, 'analyzer_cm.csv'))\n",
    "  precision_recall | io.SavePrecisionRecallCsv(\n",
    "      os.path.join(output_dir, 'analyzer_pr.csv'))\n",
    "  (logloss\n",
    "   | 'WriteLogLoss'\n",
    "   >> beam.io.WriteToText(os.path.join(output_dir,\n",
    "                                       'analyzer_logloss'),\n",
    "                         file_name_suffix='.csv'))\n",
    "\n",
    "  return confusion_matrix, precision_recall, logloss\n",
    "\n",
    "\n",
    "def get_pipeline_name(cloud):\n",
    "  if cloud:\n",
    "    return 'DataflowRunner'\n",
    "  else:\n",
    "    return  'DirectRunner'\n",
    "\n",
    "def main(argv=None):\n",
    "  \"\"\"Run Preprocessing, Training, Eval, and Prediction as a single Dataflow.\"\"\"\n",
    "  args = parse_arguments(sys.argv if argv is None else argv)\n",
    "\n",
    "  print 'Building', TRAINER_NAME, 'package.'\n",
    "  subprocess.check_call(['python', 'setup.py', 'sdist', '--format=gztar'])\n",
    "  subprocess.check_call(['gsutil', '-q', 'cp',\n",
    "                         os.path.join('dist', TRAINER_NAME), args.trainer_uri])\n",
    "  opts = None\n",
    "  if args.cloud:\n",
    "    options = {\n",
    "        'staging_location': os.path.join(args.output_dir, 'tmp', 'staging'),\n",
    "        'temp_location': os.path.join(args.output_dir, 'tmp', 'staging'),\n",
    "        'job_name': ('cloud-ml-sample-iris' + '-' +\n",
    "                     datetime.datetime.now().strftime('%Y%m%d%H%M%S')),\n",
    "        'project': args.project_id,\n",
    "        # Dataflow needs a copy of the version of the cloud ml sdk that\n",
    "        # is being used.\n",
    "        'extra_packages': [ml.sdk_location, args.trainer_uri],\n",
    "        'save_main_session': True\n",
    "    }\n",
    "    if args.sdk_location:\n",
    "      options['sdk_location'] = args.sdk_location\n",
    "    opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "  else:\n",
    "    # For local runs, the trainer must be installed as a module.\n",
    "    subprocess.check_call(['pip', 'install', '--upgrade', '--force-reinstall',\n",
    "                           '--user', os.path.join('dist', TRAINER_NAME)])\n",
    "\n",
    "  p = beam.Pipeline(get_pipeline_name(args.cloud), options=opts)\n",
    "\n",
    "  # Every function below writes its ouput to a file. The inputs to these\n",
    "  # functions are also optional; if they are missing, the input values are read\n",
    "  # from a file. Therefore if running this script multiple times, some steps can\n",
    "  # be removed to prevent recomputing values.\n",
    "  metadata, train_features, eval_features, predict_features = (\n",
    "      preprocess(p,\n",
    "                 args.training_data,\n",
    "                 args.eval_data,\n",
    "                 args.predict_data,\n",
    "                 args.output_dir))\n",
    "\n",
    "  train_args_dict = get_train_parameters(args.trainer_uri, args.endpoint,\n",
    "                                         metadata, args.trainer_job_args)\n",
    "  trained_model, results = train(p, args.output_dir, train_args_dict,\n",
    "                                 train_features, eval_features, metadata)\n",
    "\n",
    "  evaluations = evaluate(p, args.output_dir, trained_model, eval_features)\n",
    "\n",
    "  confusion_matrix, precision_recall, logloss = (\n",
    "      model_analysis(p, args.output_dir, evaluations, metadata))\n",
    "\n",
    "  if args.cloud:\n",
    "    deployed = deploy_model(p, args.output_dir, args.endpoint,\n",
    "                            args.deploy_model_name,\n",
    "                            args.deploy_model_version, trained_model)\n",
    "    # Use our deployed model to run a batch prediction.\n",
    "    output_uri = os.path.join(args.output_dir, 'batch_prediction_results')\n",
    "    deployed | 'Batch Predict' >> ml.Predict(\n",
    "        [args.predict_data],\n",
    "        output_uri,\n",
    "        region='us-central1',\n",
    "        data_format='TEXT',\n",
    "        cloud_ml_endpoint=args.endpoint)\n",
    "\n",
    "    print 'Deploying %s version: %s' % (args.deploy_model_name,\n",
    "                                        args.deploy_model_version)\n",
    "\n",
    "  p.run().wait_until_finish()\n",
    "\n",
    "  if args.cloud:\n",
    "    print 'Deployed %s version: %s' % (args.deploy_model_name,\n",
    "                                       args.deploy_model_version)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
