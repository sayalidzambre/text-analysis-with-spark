{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "# provide path to your spark directory directly\n",
    "findspark.init(\"/home/canwill/spark2\")\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "sc = pyspark.SparkContext(appName=\"elasticspark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import SQLContext\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "conf = {\"es.resource\" : \"ccthinclient/thin_client\", \"es.nodes\" : \"35.167.139.79\", \"es.port\" : \"9200\", \"es.query\" : \"?q=*\", \"es.nodes.client.only\" : \"true\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"ElasticSearch Spark Python Example\") \\\n",
    "    .config(\"spark.executor.memory\", \"5g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "es_read_conf = {\"es.resource\" : \"ccthinclient/thin_client\", \"es.nodes\" : \"35.167.139.79\", \"es.port\" : \"9200\"}\n",
    "\n",
    "es_rdd = spark.sparkContext.newAPIHadoopRDD(\n",
    "    inputFormatClass=\"org.elasticsearch.hadoop.mr.EsInputFormat\",\n",
    "    keyClass=\"org.apache.hadoop.io.NullWritable\", \n",
    "    valueClass=\"org.elasticsearch.spark.sql\", \n",
    "    conf= es_read_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "# provide path to your spark directory directly\n",
    "findspark.init(\"/home/canwill/spark20\")\n",
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "conf = SparkConf().setAppName(\"ESTest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(conf=conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "es_read_conf = {\n",
    "    \"es.nodes\" : \"localhost\",\n",
    "    \"es.port\" : \"9200\",\n",
    "    \"es.resource\" : \"megacorp/employees\",\n",
    "    \"spark.es.nodes.wan.only\":\"true\"\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "es_rdd = sc.newAPIHadoopRDD(\n",
    "    inputFormatClass=\"org.elasticsearch.hadoop.mr.EsInputFormat\",\n",
    "    keyClass=\"org.apache.hadoop.io.NullWritable\", \n",
    "    valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\", \n",
    "    conf=es_read_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc = es_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q=\"?q=taggedbyadmin:true\"\n",
    "\n",
    "es_read_conf = {\n",
    "    \"es.nodes\" : \"35.167.139.79\",\n",
    "    \"es.port\" : \"9200\",\n",
    "    \"es.resource\" : \"ccthinclient/thin_client\",\n",
    "    \"es.query\" : q\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "es_rdd = sc.newAPIHadoopRDD(\n",
    "    inputFormatClass=\"org.elasticsearch.hadoop.mr.EsInputFormat\",\n",
    "    keyClass=\"org.apache.hadoop.io.NullWritable\", \n",
    "    valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\", \n",
    "    conf=es_read_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "es_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlContext.createDataFrame(es_rdd).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.newAPIHadoopRDD(\"org.elasticsearch.hadoop.mr.EsInputFormat\",\\\n",
    "&nbsp; &nbsp; \"org.apache.hadoop.io.NullWritable\", \"org.elasticsearch.hadoop.mr.LinkedMapWritable\", conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_script = time.time()\n",
    "print ''\n",
    "print \"#######################################################################################\"\n",
    "print \"Execution Started............\"\n",
    "\n",
    "def timeit(func):\n",
    "    \"\"\"\n",
    "    Simple timing decorator\n",
    "    \"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start  = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        delta  = time.time() - start\n",
    "        return result, delta\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def identity(arg):\n",
    "    \"\"\"\n",
    "    Simple identity function works as a passthrough.\n",
    "    \"\"\"\n",
    "    return arg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from elasticsearch_dsl import DocType, String, Date, Integer\n",
    "from elasticsearch_dsl.connections import connections\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch_dsl import Search, Q\n",
    "import requests\n",
    "\n",
    "index = 'ccthinclient'\n",
    "doc_type = 'thin_client'\n",
    "\n",
    "elasticsearch_host = '35.167.139.79:9200'\n",
    "elastic_url = 'http://35.167.139.79:9200/_bulk?pretty=true'\n",
    "\n",
    "#elasticsearch_host = '52.24.198.221:8080'\n",
    "#elastic_url = 'http://52.24.198.221:8080/_bulk?pretty=true'\n",
    "\n",
    "\n",
    "# Define a default Elasticsearch client\n",
    "client = connections.create_connection(hosts=[elasticsearch_host])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index = 'ccthinclient'\n",
    "doc_type = 'thin_client'\n",
    "@timeit\n",
    "def read_elasticsearch_data(to_i):\n",
    "    body = {\n",
    "                \"from\": 0,\n",
    "                \"size\": to_i,\n",
    "                \"query\": {\n",
    "                    \"match\":{\n",
    "                        \"taggedbyadmin\":\"true\"\n",
    "                      }\n",
    "                    \n",
    "                }\n",
    "            }\n",
    "    s = Search.from_dict(body)\n",
    "    s = s.index(index)\n",
    "    s = s.doc_type(doc_type)\n",
    "    s.using(client)\n",
    "    return s.execute()\n",
    "print 'Reading train documents.........'\n",
    "raw_train, secs = read_elasticsearch_data(10)\n",
    "print 'Time taken to read',  10 ,'train document is : ', secs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "# provide path to your spark directory directly\n",
    "findspark.init(\"/home/canwill/spark2\")\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import SQLContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"ElasticSearch Spark Python Example\") \\\n",
    "    .config(\"spark.executor.memory\", \"5g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "q='''{\n",
    "                \"from\": 0,\n",
    "                \"size\": 20,\n",
    "                \"query\": {\n",
    "                    \"match\":{\n",
    "                        \"taggedbyadmin\":\"true\"\n",
    "                      }\n",
    "                    \n",
    "                }\n",
    "}'''\n",
    "\"\"\"\n",
    "\n",
    "q = '''?pretty'''\n",
    "es_read_conf = {\n",
    "    \"es.nodes\" : \"localhost\",\n",
    "    \"es.port\" : \"9200\",\n",
    "#    \"es.resource\" : \"ccthinclient/thin_client\",\n",
    "    \"es.query\" : q\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "es_rdd = spark.sparkContext.newAPIHadoopRDD(\n",
    "    inputFormatClass=\"org.elasticsearch.hadoop.mr.EsInputFormat\",\n",
    "    keyClass=\"org.apache.hadoop.io.NullWritable\", \n",
    "    valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\", \n",
    "    conf=es_read_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "# provide path to your spark directory directly\n",
    "findspark.init(\"/home/canwill/spark2\")\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import SQLContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorframes as tfs\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"ElasticSearch Spark Python Example\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ssc = StreamingContext(spark.sparkContext, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = ssc.socketTextStream(\"localhost\", 9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split each line into words\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Count each word in each batch\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "wordCounts = pairs.reduceByKey(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print the first ten elements of each RDD generated in this DStream to the console\n",
    "wordCounts.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ssc.start()             # Start the computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ssc.awaitTermination() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
