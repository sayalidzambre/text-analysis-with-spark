{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages databricks:tensorframes:0.2.7-s_2.11 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections # For frequency counting\n",
    "import findspark\n",
    "findspark.init(\"/home/canwill/spark2/\")\n",
    "\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named tensorframes",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-473cc8bce166>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorframes\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named tensorframes"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Implementation of the K-Means algorithm, while distributing the computations on a cluster.\n",
    "Given a set of feature vectors, this algorithm runs the K-Means clustering algorithm starting\n",
    "from a given set of centroids.\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorframes as tfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf_compute_distances(points, start_centers):\n",
    "    \"\"\"\n",
    "    Given a set of points and some centroids, computes the distance from each point to each\n",
    "    centroid.\n",
    "    :param points: a 2d TF tensor of shape num_points x dim\n",
    "    :param start_centers: a numpy array of shape num_centroid x dim\n",
    "    :return: a TF tensor of shape num_points x num_centroids\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(\"distances\"):\n",
    "        # The dimensions in the problem\n",
    "        (num_centroids, _) = np.shape(start_centers)\n",
    "        # The shape of the block is extracted as a TF variable.\n",
    "        num_points = tf.shape(points)[0]\n",
    "        # The centers are embedded in the TF program.\n",
    "        centers = tf.constant(start_centers)\n",
    "        # Computation of the minimum distance. This is a standard implementation that follows\n",
    "        # what MLlib does.\n",
    "        squares = tf.reduce_sum(tf.square(points), reduction_indices=1)\n",
    "        center_squares = tf.reduce_sum(tf.square(centers), reduction_indices=1)\n",
    "        prods = tf.matmul(points, centers, transpose_b = True)\n",
    "        # This code simply expresses two outer products: center_squares * ones(num_points)\n",
    "        # and ones(num_centroids) * squares\n",
    "        t1a = tf.expand_dims(center_squares, 0)\n",
    "        t1b = tf.stack([num_points, 1])\n",
    "        t1 = tf.tile(t1a, t1b)\n",
    "        t2a = tf.expand_dims(squares, 1)\n",
    "        t2b = tf.stack([1, num_centroids])\n",
    "        t2 = tf.tile(t2a, t2b)\n",
    "        distances = t1 + t2 - 2 * prods\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_one_step(dataframe, start_centers):\n",
    "    \"\"\"\n",
    "    Performs one iteration of K-Means.\n",
    "    This function takes a dataframe with dense feature vectors, a set of centroids, and returns\n",
    "    a new set of centroids along with the total distance of points to centroids.\n",
    "    This function calculates for each point the closest centroid and then aggregates the newly\n",
    "    formed clusters to find the new centroids.\n",
    "    This function uses Spark to distribute the aggregation amongst the node.\n",
    "    :param dataframe: a dataframe containing a column of features (an array of doubles)\n",
    "    :param start_centers: a k x m matrix with k the number of centroids and m the number of features\n",
    "    :return: a k x m matrix, and a positive double\n",
    "    \"\"\"\n",
    "    # The dimensions in the problem\n",
    "    (num_centroids, num_features) = np.shape(start_centers)\n",
    "    # For each feature vector, compute the nearest centroid and the distance to that centroid.\n",
    "    # The index of the nearest centroid is stored in the 'indexes' column.\n",
    "    # We also add a column of 1's that will be reduced later to count the number of elements in\n",
    "    # each cluster.\n",
    "    with tf.Graph().as_default() as g:\n",
    "        # The placeholder for the input: we use the block format\n",
    "        points = tf.placeholder(tf.double, shape=[None, num_features], name='features')\n",
    "        # The shape of the block is extracted as a TF variable.\n",
    "        num_points = tf.stack([tf.shape(points)[0]], name=\"num_points\")\n",
    "        distances = tf_compute_distances(points, start_centers)\n",
    "        # The outputs of the program.\n",
    "        # The closest centroids are extracted.\n",
    "        indexes = tf.argmin(distances, 1, name='indexes')\n",
    "        # This could be done based on the indexes as well.\n",
    "        min_distances = tf.reduce_min(distances, 1, name='min_distances')\n",
    "        counts = tf.tile(tf.constant([1]), num_points, name='count')\n",
    "        df2 = tfs.map_blocks([indexes, counts, min_distances], dataframe)\n",
    "    # Perform the reduction: we regroup the points by their centroid indexes.\n",
    "    gb = df2.groupBy(\"indexes\")\n",
    "    with tf.Graph().as_default() as g:\n",
    "        # Look at the documentation of tfs.aggregate for the naming conventions of the placeholders.\n",
    "        x_input = tfs.block(df2, \"features\", tf_name=\"features_input\")\n",
    "        count_input = tfs.block(df2, \"count\", tf_name=\"count_input\")\n",
    "        md_input = tfs.block(df2, \"min_distances\", tf_name=\"min_distances_input\")\n",
    "        # Each operation is just the sum.\n",
    "        x = tf.reduce_sum(x_input, [0], name='features')\n",
    "        count = tf.reduce_sum(count_input, [0], name='count')\n",
    "        min_distances = tf.reduce_sum(md_input, [0], name='min_distances')\n",
    "        df3 = tfs.aggregate([x, count, min_distances], gb)\n",
    "    # Get the new centroids\n",
    "    df3_c = df3.collect()\n",
    "    # The new centroids.\n",
    "    new_centers = np.array([np.array(row.features) / row['count'] for row in df3_c])\n",
    "    total_distances = np.sum([row['min_distances'] for row in df3_c])\n",
    "    return (new_centers, total_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_one_step2(dataframe, start_centers):\n",
    "    \"\"\"\n",
    "    Performs one iteration of K-Means.\n",
    "    This function takes a dataframe with dense feature vectors, a set of centroids, and returns\n",
    "    a new set of centroids along with the total distance of points to centroids.\n",
    "    This function calculates for each point the closest centroid and then aggregates the newly\n",
    "    formed clusters to find the new centroids.\n",
    "    This function performs most of the aggregation in TensorFlow.\n",
    "    :param dataframe: a dataframe containing a column of features (an array of doubles)\n",
    "    :param start_centers: a k x m matrix with k the number of centroids and m the number of features\n",
    "    :return: a k x m matrix, and a positive double\n",
    "    \"\"\"\n",
    "    # The dimensions in the problem\n",
    "    (num_centroids, _) = np.shape(start_centers)\n",
    "    # For each feature vector, compute the nearest centroid and the distance to that centroid.\n",
    "    # The index of the nearest centroid is stored in the 'indexes' column.\n",
    "    # We also add a column of 1's that will be reduced later to count the number of elements in\n",
    "    # each cluster.\n",
    "    with tf.Graph().as_default() as g:\n",
    "        # The placeholder for the input: we use the block format\n",
    "        points = tf.placeholder(tf.double, shape=[None, num_features], name='features')\n",
    "        # The distances\n",
    "        distances = tf_compute_distances(points, start_centers)\n",
    "        # The rest of this block performs a pre-aggregation step in TF, to limit the\n",
    "        # communication between TF and Spark.\n",
    "        # The closest centroids are extracted.\n",
    "        indexes = tf.argmin(distances, 1, name='indexes')\n",
    "        min_distances = tf.reduce_min(distances, 1, name='min_distances')\n",
    "        num_points = tf.stack([tf.shape(points)[0]], name=\"num_points\")\n",
    "        counts = tf.tile(tf.constant([1]), num_points, name='count')\n",
    "        # These compute the aggregate based on the indexes.\n",
    "        block_points = tf.unsorted_segment_sum(points, indexes, num_centroids, name=\"block_points\")\n",
    "        block_counts = tf.unsorted_segment_sum(counts, indexes, num_centroids, name=\"block_counts\")\n",
    "        block_distances = tf.reduce_sum(min_distances, name=\"block_distances\")\n",
    "        # One leading dimension is added to express the fact that the previous elements are just\n",
    "        # one row in the final dataframe.\n",
    "        # The final dataframe has one row per block.\n",
    "        agg_points = tf.expand_dims(block_points, 0, name=\"agg_points\")\n",
    "        agg_counts = tf.expand_dims(block_counts, 0, name=\"agg_counts\")\n",
    "        agg_distances = tf.expand_dims(block_distances, 0, name=\"agg_distances\")\n",
    "        # Using trimming to drop the original data (we are just returning one row of data per\n",
    "        # block).\n",
    "        df2 = tfs.map_blocks([agg_points, agg_counts, agg_distances],\n",
    "                             dataframe, trim=True)\n",
    "    # Now we simply collect and sum the elements\n",
    "    with tf.Graph().as_default() as g:\n",
    "        # Look at the documentation of tfs.aggregate for the naming conventions of the placeholders.\n",
    "        x_input = tf.placeholder(tf.double,\n",
    "                                 shape=[None, num_centroids, num_features],\n",
    "                                 name='agg_points_input')\n",
    "        count_input = tf.placeholder(tf.int32,\n",
    "                                     shape=[None, num_centroids],\n",
    "                                     name='agg_counts_input')\n",
    "        md_input = tf.placeholder(tf.double,\n",
    "                                  shape=[None],\n",
    "                                  name='agg_distances_input')\n",
    "        # Each operation is just the sum.\n",
    "        x = tf.reduce_sum(x_input, [0], name='agg_points')\n",
    "        count = tf.reduce_sum(count_input, [0], name='agg_counts')\n",
    "        min_distances = tf.reduce_sum(md_input, [0], name='agg_distances')\n",
    "        (x_, count_, total_distances) = tfs.reduce_blocks([x, count, min_distances], df2)\n",
    "    # The new centers\n",
    "    new_centers = (x_.T / (count_ + 1e-7)).T\n",
    "    return (new_centers, total_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kmeanstf(dataframe, init_centers, num_iters = 5, tf_aggregate = True):\n",
    "    \"\"\"\n",
    "    Runs the K-Means algorithm on a set of feature points.\n",
    "    This function takes a dataframe with dense feature vectors, a set of centroids, and returns\n",
    "    a new set of centroids along with the total distance of points to centroids.\n",
    "    :param dataframe: a dataframe containing a column of features (an array of doubles)\n",
    "    :param init_centers: the centers to start from\n",
    "    :param num_iters:  the maximum number of iterations to run\n",
    "    :return: a k x m matrix, and a list of positive doubles\n",
    "    \"\"\"\n",
    "    step_fun = run_one_step2 if tf_aggregate else run_one_step\n",
    "    c = init_centers\n",
    "    d = np.Inf\n",
    "    ds = []\n",
    "    for i in range(num_iters):\n",
    "        (c1, d1) = step_fun(dataframe, c)\n",
    "        print(\"Step =\", i, \", overall distance = \", d1)\n",
    "        c = c1\n",
    "        if d == d1:\n",
    "            break\n",
    "        d = d1\n",
    "        ds.append(d1)\n",
    "    return c, ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here is a an example of usage:\n",
    "try:\n",
    "    sc.setLogLevel('INFO')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans, KMeansModel\n",
    "from pyspark.ml.linalg import VectorUDT, _convert_to_vector\n",
    "from pyspark.sql.types import Row, StructField, StructType\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Small vectors\n",
    "num_features = 100\n",
    "# The number of clusters\n",
    "k = 10\n",
    "num_points = 100000\n",
    "num_iters = 10\n",
    "FEATURES_COL = \"features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorframes as tfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "np_data = [x.tolist() for x in np.random.uniform(0.0, 1.0, size=(num_points, num_features))]\n",
    "schema = StructType([StructField(FEATURES_COL, VectorUDT(), False)])\n",
    "mllib_rows = [Row(_convert_to_vector(x)) for x in np_data]\n",
    "mllib_df = sqlContext.createDataFrame(mllib_rows, schema).coalesce(1).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.createDataFrame([[r] for r in np_data]).toDF(FEATURES_COL).coalesce(1)\n",
    "# For now, analysis is still required. We cache the output because we are going to perform\n",
    "# multiple runs on the dataset.\n",
    "df0 = tfs.analyze(df).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mllib_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df0.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "init_centers = np.random.randn(k, num_features)\n",
    "start_centers = init_centers\n",
    "dataframe = df0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ta_0 = time.time()\n",
    "kmeans = KMeans().setK(k).setSeed(1).setFeaturesCol(FEATURES_COL).setInitMode(\n",
    "        \"random\").setMaxIter(num_iters)\n",
    "mod = kmeans.fit(mllib_df)\n",
    "ta_1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tb_0 = time.time()\n",
    "(centers, agg_distances) = kmeanstf(df0, init_centers, num_iters=num_iters, tf_aggregate=False)\n",
    "tb_1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tc_0 = time.time()\n",
    "(centers, agg_distances) = kmeanstf(df0, init_centers, num_iters=num_iters, tf_aggregate=True)\n",
    "tc_1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mllib_dt = ta_1 - ta_0\n",
    "tf_dt = tb_1 - tb_0\n",
    "tf2_dt = tc_1 - tc_0\n",
    "\n",
    "print(\"mllib:\", mllib_dt, \"tf+spark:\",tf_dt, \"tf:\",tf2_dt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
